{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "Our goal is to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList https://myanimelist.net. There is no provided dataset, so we create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import codecs\n",
    "import lxml\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "We start from the list of animes to include in the corpus of documents the search engine will work on. In particular, we focus on the top animes ever list: https://myanimelist.net/topanime.php.  The list is long and splitted in many pages. The first thing we will do is to retrieve the urls (and the names) of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list. At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `BeautifulSoup` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE links.txt EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (some minutes)\n",
    "\n",
    "# open an empty .txt file to store the urls we need\n",
    "links_text = open(\"links.txt\", \"w\")\n",
    "\n",
    "# go page by page in the site and scrap the urls we need\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                data = link.get('href')\n",
    "                # write the scrapped urls in the .txt file with '\\n' at the end of each raw\n",
    "                links_text.write(data)\n",
    "                links_text.write(\"\\n\")\n",
    "\n",
    "# close the .txt file\n",
    "links_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5089af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the number of lines in the .txt file\n",
    "file = open(\"links.txt\", \"r\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print('There are total {} lines in this file.'.format(line_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY TREE ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite some time (>25 hours)\n",
    "# REMARK: there is an issue with high frequency site-connections that blocks most of the page requests \n",
    "# a time delay between page requests has been included to solve that issue\n",
    "\n",
    "\n",
    "file = open(\"links.txt\", \"r\")\n",
    "lines = file.read().split('\\n')\n",
    "file.close()\n",
    "# returns current working directory\n",
    "base = os.getcwd()  \n",
    "# initialize the number of the first directory to be created\n",
    "t = 0\n",
    "# we use the previously created list of lines to get the urls we need\n",
    "scrapped_urls = lines[0:-1]\n",
    "for i in range(len(scrapped_urls)):\n",
    "    if(i%50==0):\n",
    "        # create a new folder\n",
    "        # remark: the newley created pages will start from 0\n",
    "        page_identifier = i-(49*t)\n",
    "        # subdirectory \n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "        # parent directories\n",
    "        parent_dir = base\n",
    "        # path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        # make directory\n",
    "        os.makedirs(path)\n",
    "        # checkpoint\n",
    "        # print(\"Directory '%s' created\" %directory)\n",
    "        # change directory \n",
    "        os.chdir(path)\n",
    "        t += 1\n",
    "\n",
    "    # to avoid the issue with high frequency site-connections  \n",
    "    time.sleep(5)   \n",
    "\n",
    "    # get urls\n",
    "    URL = scrapped_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    # parsing\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # saving\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "        \n",
    "    # checkpoint\n",
    "    # print(f\"Article {i} successfully written!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbdc2e",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ab9ff",
   "metadata": {},
   "source": [
    "At this point we have all the html documents about the animes of interest and we can start to extract the animes informations:\n",
    "- Anime Name (to save as `animeTitle`): String\n",
    "-Anime Type (to save as `animeType`): String\n",
    "-Number of episode (to save as `animeNumEpisode`): Integer\n",
    "-Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "-Number of members (to save as `animeNumMembers`): Integer\n",
    "-Score (to save as `animeScore`): Float\n",
    "-Users (to save as `animeUsers`): Integer\n",
    "-Rank (to save as `animeRank`): Integer\n",
    "-Popularity (to save as `animePopularity`): Integer\n",
    "-Synopsis (to save as `animeDescription`): String\n",
    "-Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "-Characters (to save as `animeCharacters`): List of strings.\n",
    "-Voices (to save as `animeVoices`): List of strings\n",
    "-Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd01b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "\n",
    "# Needed for the dates part\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def parse_function(path):\n",
    "    \"\"\"\n",
    "    Function that extracts anime's informations.\n",
    "    Input: path (a string that is related to the position of each anime page in the folder tree)\n",
    "    Output: a list of lists with all the informations mentioned above\n",
    "    \"\"\"\n",
    "    # take article_i.html from the directory tree we previously created\n",
    "    file = codecs.open(path, \"r\", \"utf-8\")\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # TITLES\n",
    "    # a similar code can be used for the other informations\n",
    "    animeTitle.append(soup.find_all('strong')[0].contents[0])\n",
    "    \n",
    "    divs = soup.find_all(\"div\", class_=\"spaceit_pad\")\n",
    "    \n",
    "    # DATES\n",
    "    for div in divs:\n",
    "        if div.find(\"span\", string='Aired:') != None:\n",
    "            #dates = div.contents[2].strip().split(\"to\")\n",
    "            #release = datetime.strptime(dates[0].strip(), '%b %d, %Y').date()\n",
    "            #end = datetime.strptime(dates[1].strip(), '%b %d, %Y').date()\n",
    "            \n",
    "            # check\n",
    "            # print(releaseDate)\n",
    "            aired = div.contents[2].strip().split(\"to\")\n",
    "            releaseDate = datetime.datetime(int(aired[0].split()[2]), months.index(aired[0].split()[0])+1 , int(aired[0].split()[1][:1]))\n",
    "            if len(aired) == 2:\n",
    "                endDate = datetime.datetime(int(aired[1].split()[2]), months.index(aired[1].split()[0])+1 , int(aired[1].split()[1][:1]))\n",
    "            else:\n",
    "                endDate = \"?\"\n",
    "\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            \n",
    "            # TYPES\n",
    "            if span.contents[0] == 'Type:':\n",
    "                animeType.append(div.find_all('a')[0].contents[0])\n",
    "           \n",
    "            # NUMBER OF EPISODES\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                animeNumEpisode.append(int(div.contents[2]))\n",
    "\n",
    "    # similar to what was done before\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        # MEMBERS\n",
    "        # center of the html page\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        \n",
    "        # SCORE\n",
    "        # center of the html page\n",
    "        rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "        animeScore.append(float(rating.text.strip()))\n",
    "    \n",
    "        # USERS\n",
    "        # center of the html page\n",
    "        users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "        # here we we eliminate the word 'user '   \n",
    "        # that is why there is the [:-6] part\n",
    "        # we also replace the comma divisor\n",
    "        animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        \n",
    "        # RANK\n",
    "        # center of the html page\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "        # POPULARITY\n",
    "        # center of the html page\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "    \n",
    "    # DESCRIPTION\n",
    "    # center of the html page\n",
    "    animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "     \n",
    "    \n",
    "    # RELATED \n",
    "    # center of the html page\n",
    "    x = []\n",
    "    y = []\n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    for tr in related:\n",
    "        td = tr.find_all(\"td\")\n",
    "        for i in range(0, len(td), 2):\n",
    "            x.append(td[i].contents[0])\n",
    "            t = td[i+1].find_all(\"a\")\n",
    "            y.append(t[0].contents[0])\n",
    "        animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "    \n",
    "    \n",
    "    # CHARACTERS\n",
    "    # center of the html page (bottom\n",
    "    characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "    x = []\n",
    "    for i in chars:\n",
    "        x.append(i.contents[0].contents[0])\n",
    "    animeCharacters.append(x)\n",
    "    \n",
    "    \n",
    "    # VOICES\n",
    "    # center of the html page (bottom)\n",
    "    voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "    y = []\n",
    "    for i in voices:\n",
    "        y.append(i.contents[1].contents[0])\n",
    "    animeVoices.append(y)\n",
    "\n",
    "    \n",
    "    # STAFF\n",
    "    # center of the html page (bottom)\n",
    "    if(len(soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})) == 2):\n",
    "        staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        staff = staff[1].find_all(\"td\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(1, len(staff), 2):\n",
    "            x.append(staff[i].contents[1].contents[0])\n",
    "            y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "        animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "        \n",
    "    return [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers,\n",
    "               animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \n",
    "               animeCharacters, animeVoices, animeStaff]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3d1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fullmetal Alchemist: Brotherhood'], ['TV'], [64], datetime.datetime(2009, 4, 5, 0, 0), datetime.datetime(2010, 7, 4, 0, 0), [2675751], [9.16], [1622384], [1], [3], 'After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse\\'s body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse\\'s soul in the physical realm by binding it to a hulking suit of armor.\\rThe brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing \"automail,\" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher\\'s Stone—a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.\\rAs Edward becomes an infamous alchemist and gains the nickname \"Fullmetal,\" the boys\\' journey embroils them in a growing conspiracy that threatens the fate of the world.\\r[Written by MAL Rewrite]', [['Adaptation: Fullmetal Alchemist', 'Alternative version: Fullmetal Alchemist', 'Side story: Fullmetal Alchemist: Brotherhood Specials', 'Spin-off: Fullmetal Alchemist: Brotherhood - 4-Koma Theater']], [['Elric, Edward', 'Elric, Alphonse', 'Mustang, Roy', 'Hughes, Maes', 'Greed', 'Hawkeye, Riza', 'Yao, Ling', 'Armstrong, Alex Louis', 'Rockbell, Winry', 'Armstrong, Olivier Mira']], [['Park, Romi', 'Kugimiya, Rie', 'Miki, Shinichiro', 'Fujiwara, Keiji', 'Nakamura, Yuuichi', 'Orikasa, Fumiko', 'Miyano, Mamoru', 'Utsumi, Kenji', 'Takamoto, Megumi', 'Soumi, Youko']], [[['Cook, Justin', 'Producer'], ['Yonai, Noritomo', 'Producer'], ['Irie, Yasuhiro', 'Director, Episode Director, Storyboard'], ['Mima, Masafumi', 'Sound Director']]]]\n"
     ]
    }
   ],
   "source": [
    "# create a for loop ro adjurn the path in order to call the function several times\n",
    "# and get all the articles\n",
    "# Also a good idea would be to put this function in a python file and then import it\n",
    "# so the code would be cleaner\n",
    "path = 'html_pages/page_0.html/article_0.html'\n",
    "print(parse_function(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7e1d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with dates parsing in tsv file\n",
    "# if you pass something like str(releaseDate) you get the following error\n",
    "# NameError: name 'releaseDate' is not defined\n",
    "with open('anime_'+str(0)+'.tsv', 'wt') as file:\n",
    "    file.write(str(animeTitle)+'\\t' + str(animeType)+'\\t' + str(animeNumEpisode)+'\\t' + 'releaseDate' +'\\t' + \n",
    "               'endDate'+'\\t' + str(animeNumMembers)+'\\t' + str(animeScore)+'\\t' + str(animeUsers)\n",
    "              +'\\t' + str(animeRank)+'\\t' + str(animePopularity)+'\\t' + str(animeDescription)+'\\t' + \n",
    "               str(animeRelated)+'\\t' + str(animeCharacters)+'\\t' + str(animeVoices)+'\\t' + str(animeStaff))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aac4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

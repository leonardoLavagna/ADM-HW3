{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8756d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to install\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm # useful for progress bars\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list (cfr. https://myanimelist.net/topanime.php). At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `selenium` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debbf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9204e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service('/Users/dany/Desktop/adm-hw3/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium with Chrome\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aa4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with links of each anime\n",
    "df = pd.DataFrame(columns = ['Href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6192e2",
   "metadata": {},
   "source": [
    "The following code was inspired by looking at the work that was done last year about https://www.goodreads.com, for example by https://github.com/GiorgiaSalvatori/ADM-HW3/blob/main/main.ipynb. Also the following post was useful https://towardsdatascience.com/how-to-use-selenium-to-web-scrape-with-example-80f9b23a843a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f107bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [06:54<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# go page by page and and store links in a list\n",
    "anime_list = []\n",
    "\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime_list.append((link.contents[0], link.get('href')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d7dbe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19124\n"
     ]
    }
   ],
   "source": [
    "# total number of animes\n",
    "print(len(anime_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38884487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign list to dataframe\n",
    "df['Href'] = anime_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442da9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19123"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicates: ok no duplicates\n",
    "df['Href'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ed84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe into a csv file without header and comma separator. \n",
    "# this is equivalent to a txt file, but with also the names of the animes\n",
    "# that can be of help in some data processing stages. \n",
    "df.to_csv('urls.csv',sep = ' ', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0bbbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (Fullmetal Alchemist: Brotherhood, https://mya...\n",
       "1    (Gintama°, https://myanimelist.net/anime/28977...\n",
       "2    (Shingeki no Kyojin Season 3 Part 2, https://m...\n",
       "3    (Steins;Gate, https://myanimelist.net/anime/92...\n",
       "4    (Fruits Basket: The Final, https://myanimelist...\n",
       "Name: Href, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Href'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc99b2",
   "metadata": {},
   "source": [
    "We could also create a dictionary, this is useful in some circumnstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b57c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood', 'https://myanimelist.net/anime/28977/Gintama°', 'https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2', 'https://myanimelist.net/anime/9253/Steins_Gate', 'https://myanimelist.net/anime/42938/Fruits_Basket__The_Final']\n"
     ]
    }
   ],
   "source": [
    "#keys\n",
    "name = []   \n",
    "#values\n",
    "url = []    \n",
    "\n",
    "for item in anime_list:\n",
    "    name.append(item[0])\n",
    "    url.append(item[1])\n",
    "    \n",
    "D = dict(zip(name, url))\n",
    "print(list(D.values())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0846fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remark: the execution can take quite some time, for this reason I will stop at the first 5 animes.\n",
    "# remark : there is an issue with high frequency site-connections \n",
    "\n",
    "# returns current working directory\n",
    "base = os.getcwd()  \n",
    "t = 0\n",
    "# we use the previously created dictionary D to get the urls we need\n",
    "scrapped_urls = list(D.values())[0:5]\n",
    "for i in range(len(scrapped_urls)):\n",
    "    if(i%50==0):\n",
    "        # create a new folder\n",
    "        # remark: the pages will start from 0\n",
    "        page_identifier = i-(49*t)\n",
    "        # subdirectory\n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "        # parent directories\n",
    "        parent_dir = base\n",
    "        # path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        # make directory\n",
    "        os.makedirs(path)\n",
    "        # checkpoint\n",
    "        #print(\"Directory '%s' created\" %directory)\n",
    "        # change directory \n",
    "        os.chdir(path)\n",
    "        t += 1\n",
    "        \n",
    "    # get urls\n",
    "    URL = scrapped_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    # parsing\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # saving\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "        \n",
    "    # checkpoint\n",
    "    #print(f\"Article {i} successfully written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c6aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

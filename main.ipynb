{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "Our goal is to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList https://myanimelist.net. There is no provided dataset, so we create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request \n",
    "import requests\n",
    "import codecs\n",
    "import csv\n",
    "import os \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961689af",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: Some parts of the following code was inspired by looking at the work that was done last year about https://www.goodreads.com, for example by https://github.com/GiorgiaSalvatori/ADM-HW3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "We start from the list of animes to include in the corpus of documents the search engine will work on. In particular, we focus on the top animes ever list: https://myanimelist.net/topanime.php.  The list is long and splitted in many pages. The first thing we will do is to retrieve the urls (and the names) of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list. At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `BeautifulSoup` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE links.txt EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (some minutes)\n",
    "\n",
    "# open an empty .txt file to store the urls we need\n",
    "links_text = open(\"links.txt\", \"w\")\n",
    "\n",
    "# go page by page in the site and scrap the urls we need\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                data = link.get('href')\n",
    "                # write the scrapped urls in the .txt file with '\\n' at the end of each raw\n",
    "                links_text.write(data)\n",
    "                links_text.write(\"\\n\")\n",
    "\n",
    "# close the .txt file\n",
    "links_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5089af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 19124 lines in this file.\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE IF AND ONLY IF THE links.txt FILE HAS BEEN CREATED\n",
    "\n",
    "# Read the number of lines in the .txt file\n",
    "file = open(\"links.txt\", \"r\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print('There are total {} lines in this file.'.format(line_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY TREE ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite some time (>25 hours)\n",
    "# REMARK: there is an issue with high frequency site-connections that blocks most of the page requests \n",
    "# a time delay between page requests can been included to solve that issue\n",
    "\n",
    "# creating the directory for html pages to be downloaded\n",
    "directory = 'html_pages'\n",
    "file_read = open('links.txt', 'r')\n",
    "anime_urls_list = file_read.readlines()\n",
    "file_read.close()\n",
    "\n",
    "for i in range(1,401):\n",
    "    # reading each page from 1 to 400\n",
    "    html_page_name = 'page'+str(i)\n",
    "    os.makedirs(os.path.join(directory, html_page_name ))\n",
    "    directory_subfolder = directory+'/'+html_page_name+'/'\n",
    "    \n",
    "    # reading each anime on the list from 1 to 50\n",
    "    for j in range(1,51):\n",
    "        anime_num = 50*(i-1)+j\n",
    "        html_file_name = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "        temp_text = open(html_file_name, \"w\")\n",
    "        url = anime_urls_list[(anime_num-1)].encode('ascii','backslashreplace').decode('utf-8')\n",
    "        urllib.request.urlretrieve(url,html_file_name)\n",
    "        temp_text.close()\n",
    "\n",
    "    # to avoid the issue with high frequency site-connections  \n",
    "    time.sleep(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbdc2e",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ab9ff",
   "metadata": {},
   "source": [
    "At this point we have all the html documents about the animes of interest and we can start to extract the animes informations:\n",
    "- Anime Name (to save as `animeTitle`): String\n",
    "- Anime Type (to save as `animeType`): String\n",
    "- Number of episode (to save as `animeNumEpisode`): Integer\n",
    "- Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "- Number of members (to save as `animeNumMembers`): Integer\n",
    "- Score (to save as `animeScore`): Float\n",
    "- Users (to save as `animeUsers`): Integer\n",
    "- Rank (to save as `animeRank`): Integer\n",
    "- Popularity (to save as `animePopularity`): Integer\n",
    "- Synopsis (to save as `animeDescription`): String\n",
    "- Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "- Characters (to save as `animeCharacters`): List of strings.\n",
    "- Voices (to save as `animeVoices`): List of strings\n",
    "- Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147674ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "\n",
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "directory = 'html_pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff507d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "\n",
    "def parse_function(html_file_path):\n",
    "    \"\"\"\n",
    "    Function that extracts anime's informations.\n",
    "    Input: path (a string that is related to the position of each anime page in the folder tree)\n",
    "    Output: a list of lists with all the informations mentioned above\n",
    "    \"\"\"\n",
    "    \n",
    "    # take article_i.html from the directory \n",
    "    soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "\n",
    "    # TITLE\n",
    "    try:\n",
    "        animeTitle.append(str(soup.find_all('strong')[0].contents[0]))\n",
    "    except:\n",
    "        animeTitle.append('')\n",
    "\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            \n",
    "            # TYPES\n",
    "            if span.contents[0] == 'Type:':\n",
    "                try:\n",
    "                    animeType.append(str(div.find_all('a')[0].contents[0]))\n",
    "                except:\n",
    "                    animeType.append('NA')\n",
    "            \n",
    "            # NUMBER OF EPISODES\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                try: \n",
    "                    animeNumEpisode.append(int(div.contents[2]))\n",
    "                except:\n",
    "                    animeNumEpisode.append(0)\n",
    "            \n",
    "            # DATES\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                try:\n",
    "                    if len(div.contents[2]) > 21:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        end = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        endDate.append(end)\n",
    "                    else:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        endDate.append('-')\n",
    "                except:\n",
    "                        releaseDate.append('')\n",
    "                        endDate.append('')\n",
    "\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        \n",
    "        # MEMBERS\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        \n",
    "        # SCORE\n",
    "        rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "        try:        \n",
    "            animeScore.append(float(rating.text.strip()))\n",
    "        except:\n",
    "            animeScore.append(None)\n",
    "\n",
    "        # USERS\n",
    "        users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "        # here we we eliminate the word 'user '   \n",
    "        # that is why there is the [:-6] part\n",
    "        # we also replace the comma divisor\n",
    "        try:\n",
    "            animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        except:\n",
    "            animeUsers.append(0)\n",
    "\n",
    "        # RANK\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        try:\n",
    "            animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "        except:\n",
    "            animeRank.append(None)\n",
    "\n",
    "        # POPULARITY\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "    # DESCRIPTION\n",
    "    animeDescription.append(soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', ''))\n",
    "\n",
    "    # RELATED \n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    if(len(related)!=0):\n",
    "        x = []\n",
    "        y = []\n",
    "        for tr in related:\n",
    "            td = tr.find_all(\"td\")\n",
    "            for i in range(0, len(td), 2):\n",
    "                x.append(td[i].contents[0])\n",
    "                t = td[i+1].find_all(\"a\")\n",
    "                if(len(t[0].contents)!=0):  \n",
    "                    y.append(t[0].contents[0])\n",
    "                else:\n",
    "                    y.append(' ')\n",
    "            animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "    else:\n",
    "        animeRelated.append(' ')\n",
    "    \n",
    "    # CHARACTERS\n",
    "    try:\n",
    "        characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "        x = []\n",
    "        for i in chars:\n",
    "            x.append(i.contents[0].contents[0])\n",
    "        animeCharacters.append(x)\n",
    "    except:\n",
    "        animeCharacters.append(\" \")\n",
    "    \n",
    "   # VOICES\n",
    "    try:\n",
    "        voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "        y = []\n",
    "        for i in voices:\n",
    "            y.append(i.contents[1].contents[0])\n",
    "        animeVoices.append(y)\n",
    "    except:\n",
    "        animeVoices.append(\" \")\n",
    "    \n",
    "    # STAFF\n",
    "    try:\n",
    "        staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        staff = staff[1].find_all(\"td\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(1, len(staff), 2):\n",
    "            x.append(staff[i].contents[1].contents[0])\n",
    "            y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "        animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "    \n",
    "    except:\n",
    "        animeStaff.append(\" \")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution should take a few seconds\n",
    "\n",
    "# create a directory tree for .tsv files\n",
    "os.mkdir('tsv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE .tsv FILES ALREADY EXIST THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "def tsv_create(i):\n",
    "    \"\"\"\n",
    "    Function that creates a .tsv file form the html anime pages\n",
    "    Input: i, a positive integer\n",
    "    Output: empty\n",
    "    RemarK: it creates a .tsv file named anime_i in the tsv_files directory\n",
    "    \"\"\"\n",
    "    tsv_columns = ['animeTitle','animeType','animeNumEpisode','releaseDate','endDate','animeNumMembers','animeScore',\n",
    "                  'animeUsers','animeRank','animePopularity','animeDescription','animeRelated','animeCharacters',\n",
    "                  'animeVoices','animeStaff']\n",
    "    data = zip([animeTitle[i-1]],[animeType[i-1]],[animeNumEpisode[i-1]],[releaseDate[i-1]],[endDate[i-1]],[animeNumMembers[i-1]],[animeScore[i-1]],[animeUsers[i-1]],[animeRank[i-1]],[animePopularity[i-1]],[animeDescription[i-1]],[animeRelated[i-1]],[animeCharacters[i-1]],[animeVoices[i-1]],[animeStaff[i-1]])\n",
    "    tsv_file_name = 'tsv_files/anime_'+str(i)+'.tsv'\n",
    "    with open(tsv_file_name, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(tsv_columns)\n",
    "        for title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff in data:\n",
    "                tsv_output.writerow([title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE (WITH A SUBSET OF PAGES) \n",
    "# IF THE DIRECTORY AND THE .tsv FILES ALREADY EXIST THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite a while (>1 hour)\n",
    "\n",
    "# for each row create a tsv file\n",
    "for i in range(1,384):\n",
    "    html_page_name = 'page'+str(i)\n",
    "    directory_subfolder = directory+'/'+html_page_name+'/'\n",
    "    if(i!=383):\n",
    "        # 383th page has less than 50 animes\n",
    "        for j in range(1,51):\n",
    "            anime_num = 50*(i-1)+j\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            tsv_create(anime_num)\n",
    "    else:\n",
    "        for j in range(1,25):\n",
    "            anime_num = 50*(i-1)+j\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            tsv_create(anime_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018a5a8",
   "metadata": {},
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d7c3b",
   "metadata": {},
   "source": [
    "We will create two different Search Engines that, given as input a query, return the animes that match the query. First, we need to pre-process all the information collected for each anime by:\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "\n",
    "For this purpose, we will use the `nltk` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c29da",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73781b",
   "metadata": {},
   "source": [
    "For the first version of the search engine, we narrow our interest on the `Synopsis` of each anime. It means that we will evaluate queries only with respect to the anime's description (and `Title` as we believe it is also an important part of an anime description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ba55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcc4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords and store them in a variable\n",
    "stop = stopwords.words('english')\n",
    "# stemmer\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe379de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    \"\"\"\n",
    "    Input: sentence, a string\n",
    "    Output: tokenized sentence\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMARK: The execution can take a from a few seconds to a couple of minutes\n",
    "\n",
    "animeTitle_list = []\n",
    "animeDescription_list = []\n",
    "\n",
    "# create a lists from tsv files\n",
    "for i in range(0,19124):\n",
    "    anime_tsv = open('tsv_files/anime_'+str(i+1)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "    data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "    data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "    animeTitle_list.append(str(data.animeTitle[0]))\n",
    "    animeDescription_list.append(str(data.animeDescription[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7f4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate lists to create a dataframe\n",
    "anime_df = pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list]), \n",
    "                               columns=['animeTitle', 'animeDescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c7755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-dc290c8ffdb2>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  anime_df['animeDescription'] = anime_df['animeDescription'].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "# REMARK: The execution can take a froma a few seconds to a couple of minutes\n",
    "\n",
    "# removing stopwords from the dataframe\n",
    "anime_df['animeDescription']  = anime_df['animeDescription'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# removing punctuations from the dataframe\n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# stemming the dataframe \n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].apply(stem_sentences)\n",
    "\n",
    "# remove [Written by MAL Rewrite]\n",
    "# e.g. text.replace('[Written by MAL Rewrite]', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34fb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = ' '.join([i for i in anime_df['animeDescription']]).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d6c38",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d15f",
   "metadata": {},
   "source": [
    "Given a query (e.g. *saiyan race*) the Search Engine returns a list of documents. Since we are dealing with conjunctive queries (AND), each of the returned documents contains all the words in the query. The final output of the query returns, if present, the following information for each of the selected documents:\n",
    "- animeTitle\n",
    "- animeDescription\n",
    "- Url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dce6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "words_dict = set(words_list)\n",
    "\n",
    "# assign a unique integer id to each unique word\n",
    "vocabulary = {}\n",
    "i=1\n",
    "for word in words_dict:\n",
    "    vocabulary.update({i:word})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca106c",
   "metadata": {},
   "source": [
    "### 2.1.1 Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE vocabulary.json FILE EXISTS DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# create the vocabulary.json file to store each unique word \n",
    "# and its corresponding id number \n",
    "with open(\"vocabulary.json\", \"w\") as file:\n",
    "    json.dump(vocabulary, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE inverted.json FILE ALREADY EXISTS DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (>1 hour)\n",
    "\n",
    "# create inverted index\n",
    "inverted_dict = {}\n",
    "with open('vocabulary.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    for key, value in tqdm(data.items()):\n",
    "        inverted_list = []\n",
    "        for i in range(0,len(anime_df)):\n",
    "            if(value in anime_df['animeDescription'][i].split()):\n",
    "                anime_name = 'anime_'+str(i+1)\n",
    "                inverted_list.append(anime_name)\n",
    "                inverted_dict.update({key:inverted_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c168e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE AFTER THE inverted.json FILE HAS BEEN CREATED \n",
    "\n",
    "# save the inverte index in a .json file\n",
    "with open(\"inverted.json\", \"w\") as file:\n",
    "    json.dump(inverted_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e7cf4",
   "metadata": {},
   "source": [
    "### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce810b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_query(query_list):\n",
    "    \"\"\"\n",
    "    Input: the user's query, a string\n",
    "    Output: a list of animes that match the query\n",
    "    \"\"\"\n",
    "    anime_query_list = []\n",
    "    for word in query_list:\n",
    "        with open('vocabulary.json') as data_file:\n",
    "            data = json.load(data_file)\n",
    "            for key, value in data.items():\n",
    "                if(word == value):\n",
    "                    with open('inverted.json') as inverted_file:\n",
    "                        inverted_data = json.load(inverted_file)\n",
    "                        for inv_key, inv_value in inverted_data.items():\n",
    "                            if(key == inv_key):\n",
    "                                # appending the value to a list if has the specific query word\n",
    "                                anime_query_list.append(inv_value)\n",
    "    \n",
    "    # creating a list from all animes including duplicate ones\n",
    "    anime_list = []\n",
    "    for i in range(len(anime_query_list)):\n",
    "        for j in range(len(anime_query_list[i])):\n",
    "            anime_list.append(anime_query_list[i][j])\n",
    "    \n",
    "    # creating a set to find non duplicate anime files\n",
    "    anime_query_set_list = list(set(anime_list))\n",
    "    # creating an empty list to store the final anime list which has all the input queries \n",
    "    anime_final_list = []\n",
    "    \n",
    "    # counting the occurences of each anime with the length of the total query\n",
    "    # if its equal to total len, then each word in the query appears on the anime description\n",
    "    for anime in anime_query_set_list:\n",
    "        if(anime_list.count(anime) == len(anime_query_list)):\n",
    "            anime_final_list.append(anime)\n",
    "    return anime_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b581352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_anime_df(anime_list):\n",
    "    \"\"\"\n",
    "    Input: a list of animes (obteined through a user's query)\n",
    "    Output: a dataframe with the title, the description and the url of all the\n",
    "            animes in the list\n",
    "    \"\"\"\n",
    "    # creating lists for animes\n",
    "    animeTitle_list = []\n",
    "    animeDescription_list = []\n",
    "    animeUrl_list = []\n",
    "\n",
    "    # assigning tsv values from the animes to lists we've just created \n",
    "    for anime in anime_list:\n",
    "        anime_tsv = open('tsv_files/'+anime+'.tsv', 'r',encoding=\"utf8\")\n",
    "        data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "        data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "        data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "        animeTitle_list.append(str(data.animeTitle[0]))\n",
    "        animeDescription_list.append(str(data.animeDescription[0]))\n",
    "\n",
    "    # reading text file url lines to a list\n",
    "    f=open('links.txt')\n",
    "    url_lines=f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # creating a for loop to iterate over each anime we have on the anime_list\n",
    "    for anime in anime_list:\n",
    "        # getting the int value from the anime name\n",
    "        anime_num=(int(anime.split(\"anime_\",1)[1]))\n",
    "        # finding the corresponding line from the links.txt and assigning it to a list\n",
    "        animeUrl_list.append(url_lines[(anime_num-1)])\n",
    "\n",
    "    # creating the dataframe from lists and returning it\n",
    "    return pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list, animeUrl_list]), \n",
    "                                   columns=['animeTitle', 'animeDescription', 'Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c13de143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search:saiyan race\n"
     ]
    }
   ],
   "source": [
    "# getting an input from the user\n",
    "# example: query = 'saiyan race'\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532f2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# getting the list of animes which has the query\n",
    "anime_list = find_query(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757fb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the anime dataframe from our query\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aed3818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "0                                      Dragon Ball Z   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "2                                    Dragon Ball Kai   \n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  Five years after winning the World Martial Art...   \n",
       "1  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "2  Five years after the events of Dragon Ball, ma...   \n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  \n",
       "1  https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "2  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the results\n",
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a9781",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d5008",
   "metadata": {},
   "source": [
    "For the second search engine, given a query, we want to get the top-k documents related to the query. In particular:\n",
    "- Find all the documents that contains all the words in the query.\n",
    "- Sort them by their similarity with the query.\n",
    "- Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. \n",
    "\n",
    "We use a heap data structure for maintaining the top-k documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3e4a5",
   "metadata": {},
   "source": [
    "### 2.2.1 Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483a1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf(word, anime_num):\n",
    "    \"\"\"\n",
    "    This function finds the number of specific word in a document\n",
    "    Input: word, a string, and anime_num the number of an anime taken from a .tsv file\n",
    "    Output: tf, a float that is the frequency of the word\n",
    "    \"\"\"\n",
    "    document = anime_df['animeDescription'][anime_num-1].split()\n",
    "    tf_counter = 0\n",
    "    for i in range(len(document)):\n",
    "        if(document[i] == word):\n",
    "            tf_counter+=1\n",
    "    tf = tf_counter / len(document)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4b6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_df(word):\n",
    "    \"\"\"\n",
    "    This function finds the number of occurences of the word in all the documents\n",
    "    Input: word, a string\n",
    "    Output: df_counter, a float that is the frequency of the word in all the documents\n",
    "    \"\"\"\n",
    "    df_counter = 0\n",
    "    for i in range(len(anime_df)):\n",
    "        document = anime_df['animeDescription'][i].split()\n",
    "        for i in range(len(document)):\n",
    "            if(document[i] == word):\n",
    "                df_counter+=1 \n",
    "    return df_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d8a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE tfidf.json EXISTS THEN DO NOT RUN THIS CELL\n",
    "\n",
    "# create the last version of the tfidf dict\n",
    "tfidf_last_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE tfidf.json EXISTS THEN DO NOT RUN THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (>10 hours)\n",
    "\n",
    "with open('inverted.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    data_file.close()\n",
    "    \n",
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()\n",
    "    \n",
    "for i in tqdm(range(len(data))):\n",
    "    # reading each anime to a list that contains a specific word\n",
    "    word = data_vocab[str(i+1)] # getting the specific word from vocabulary json\n",
    "    anime_list = data[str(i+1)] # getting all animes related to a specific word from inverted.json\n",
    "    tfidf_list = []\n",
    "    for anime in anime_list:\n",
    "        tfidf_dict = {}\n",
    "        # getting the anime num from the tsv file name\n",
    "        anime_num=(int(anime.split(\"anime_\",1)[1]))\n",
    "        # finding the tf value\n",
    "        tf = find_tf(word, anime_num) \n",
    "        # finding the df value\n",
    "        df_counter = find_df(word)\n",
    "        # inverse document frequency aka idf is a log of total documents divided by df+1\n",
    "        # idf = log(N/(df + 1))\n",
    "        idf = math.log((len(anime_df) / df_counter+1), 10)\n",
    "        # calculating the tfidf number by multiplication\n",
    "        tfidf = tf * idf\n",
    "        # creating a dictionary for each anime - tfidf pairs\n",
    "        tfidf_dict.update({anime:tfidf})\n",
    "        tfidf_list.append(tfidf_dict)\n",
    "    tfidf_last_dict.update({i+1:tfidf_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE tfidf.json EXISTS THEN DO NOT RUN THIS CELL\n",
    "\n",
    "# creating the json file\n",
    "with open(\"tfidf.json\", \"w\") as file:\n",
    "    json.dump(tfidf_last_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bdc22",
   "metadata": {},
   "source": [
    "### 2.2.2 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c240e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e54df0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Input: vec1, vec2 arrays that represent the user's query and an anime description\n",
    "    Output: cosine similarity of vec1 and vec2\n",
    "    \"\"\"\n",
    "    # the two string vectors intersection and numerator calculation\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    # summing up the values from the vectors\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    # calculating the denominator for the cosine\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    \n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        # return the cosine\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Input: text, a string\n",
    "    Output: a dictionary subclass for counting the text (hashable onject)\n",
    "    \"\"\"\n",
    "    # getting a string value and turning it to a vector \n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70e4ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search:saiyan race\n"
     ]
    }
   ],
   "source": [
    "# user's input\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4100d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming, removing stop words and punctuations from the input string\n",
    "query = ' '.join([word for word in query.split() if word not in stop])\n",
    "query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "query = stem_sentences(query)\n",
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# creating regex pattern object\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "# from the anime_list we find the query\n",
    "anime_list = find_query(query_list)\n",
    "# creating a dataframe from the anime list we got\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1929a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the anime numbers from the returned dataframe from the query\n",
    "anime_num_list = []\n",
    "for i in range(len(anime_df)):\n",
    "    for j in range(len(query_anime_df)):\n",
    "        if(anime_df['animeTitle'][i] == query_anime_df['animeTitle'][j]):\n",
    "            anime_num_list.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6654aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the tfidf json file and saving it to a dictionary\n",
    "with open('tfidf.json') as word_file:\n",
    "    data_tfidf = json.load(word_file)\n",
    "    word_file.close()\n",
    "    \n",
    "# opening the vocabulary json file and saving it to a dictionary\n",
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cccc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_score(query, anime_num):\n",
    "    \"\"\"\n",
    "    Function that calculates tfidf score (measures the importance of a term wrt a document)\n",
    "    Input: query, a string given by the user, anime_num an integer that represent an anime\n",
    "    Output: tfidf_score, average score of tfidf score\n",
    "    \"\"\"\n",
    "    tfidf_score_list = []\n",
    "    query_list = query.split()\n",
    "    # splitting the sentence to get each word\n",
    "    for word in query_list:\n",
    "        # for every word we iterate\n",
    "        for key, value in data_vocab.items():\n",
    "            # trying to find the word inside vocabulary.json file\n",
    "            if(word == value):\n",
    "                # if we find it, we match that key with tfidf json key\n",
    "                for tfidf_key, tfidf_value in data_tfidf.items():\n",
    "                    if(key == tfidf_key):\n",
    "                        # when we find the key, we find the scores for each anime\n",
    "                        for i in range(len(data_tfidf[tfidf_key])):\n",
    "                            for t_key, t_value in data_tfidf[tfidf_key][i].items():\n",
    "                                # finding the matching anime names \n",
    "                                if(anime_num == (int(t_key.split(\"anime_\",1)[1]))):\n",
    "                                    # appending the tfidf score to a list\n",
    "                                    tfidf_score_list.append(t_value)\n",
    "    # getting the average score of tfidf score\n",
    "    tfidf_score = sum(tfidf_score_list) / len(tfidf_score_list) \n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca21e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(anime_num_list, query):\n",
    "    \"\"\"\n",
    "    Input: anime_num_list, an array with the animes' numbers, query, a string with the user's query\n",
    "    Output: an array with the similarity score values \n",
    "    \"\"\"\n",
    "    similarity_score_list = []\n",
    "    # iterate over the query dataframe we created\n",
    "    for i in range(len(query_anime_df)):\n",
    "        # get the specific anime's synopsis\n",
    "        anime_synopsis = anime_df['animeDescription'][anime_num_list[i]-1]\n",
    "        # get the vectors of the query and the anime's synopsis\n",
    "        vector1 = text_to_vector(query)\n",
    "        vector2 = text_to_vector(anime_synopsis)\n",
    "        # calculating the cosine similarity\n",
    "        cosine = get_cosine(vector1, vector2)\n",
    "        # getting the average tfidf score from each query in the sentence\n",
    "        tfidf_score = get_tfidf_score(query, anime_num_list[i])\n",
    "        # finding the similarity score\n",
    "        similarity_score = cosine * tfidf_score\n",
    "        similarity_score_list.append(similarity_score)\n",
    "    return similarity_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c43db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the similarity_score\n",
    "similarity_score_list = find_similarity(anime_num_list, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8395e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending similarity score \n",
    "query_anime_df['similarity'] = similarity_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f2f80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the values based on similarity on descending order by heapsort and getting the top5 results\n",
    "query_anime_df = query_anime_df.sort_values('similarity',ascending= False,kind = 'heapsort').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1572699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.018094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.006454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.001686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "0                                      Dragon Ball Z   \n",
       "2                                    Dragon Ball Kai   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "0  Five years after winning the World Martial Art...   \n",
       "2  Five years after the events of Dragon Ball, ma...   \n",
       "\n",
       "                                                 Url  similarity  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...    0.018094  \n",
       "1  https://myanimelist.net/anime/36946/Dragon_Bal...    0.006454  \n",
       "0  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n    0.002246  \n",
       "2  https://myanimelist.net/anime/6033/Dragon_Ball...    0.001686  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af259d00",
   "metadata": {},
   "source": [
    "# 3 Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b18f97",
   "metadata": {},
   "source": [
    "For the new score metric we define as the following: $new\\_score = (similarity((animeScore/100) + 1/animePopularity))100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87e4616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_popularity_and_score_list(anime_num_list):\n",
    "    \"\"\"\n",
    "    Input: anime_num_list, an array \n",
    "    Output: anime score values and popularity (as lists)\n",
    "    \"\"\"\n",
    "    # finding the popularity and vote score values from the tsv files and appending to a list\n",
    "    animeScore_list = []\n",
    "    animePopularity_list = []\n",
    "    # for each anime we iterate over the list\n",
    "    for anime in anime_num_list:\n",
    "        # opening the tsv file\n",
    "        anime_tsv = open('tsv_files/anime_'+str(anime)+'.tsv', 'r',encoding=\"utf8\")\n",
    "        # getting the data\n",
    "        data=pd.read_table(anime_tsv)[['animeScore','animePopularity']]\n",
    "        # assigning the anime score value and popularity\n",
    "        data['animeScore'] = data['animeScore'].astype(str)\n",
    "        data['animePopularity'] = data['animePopularity'].astype(str)\n",
    "        animeScore_list.append(str(data.animeScore[0]))\n",
    "        animePopularity_list.append(str(data.animePopularity[0]))\n",
    "    \n",
    "    return animeScore_list, animePopularity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd73848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(animeScore_list, animePopularity_list):\n",
    "    \"\"\"\n",
    "    Input: the lists returned by find_popularity_and_score_list \n",
    "    Output: the weighted input lists according to the new_score\n",
    "    \"\"\"\n",
    "    for i in range(len(animeScore_list)):\n",
    "        # finding the animeScore weight by dividing it by 100\n",
    "        animeScore_list[i] = float(animeScore_list[i])/100\n",
    "        # finding the popularity score weight by dividing to 1 \n",
    "        animePopularity_list[i] = 1/(float(animePopularity_list[i]))\n",
    "    return animeScore_list,animePopularity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1ec6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(similarity_score_list, cal_animeScore, calanimePopularity):\n",
    "    \"\"\"\n",
    "    Function that calculates the new_score we defined above\n",
    "    \"\"\"\n",
    "    new_score_list = []\n",
    "    for i in range(len(similarity_score_list)):\n",
    "        new_score = (similarity_score_list[i] * (cal_animeScore[i] + calanimePopularity[i]))*100\n",
    "        new_score_list.append(new_score)\n",
    "    return new_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8f8ff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search:saiyan race\n"
     ]
    }
   ],
   "source": [
    "# getting an input from the user\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0877bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming, removing stop words and punctuations from the input string\n",
    "query = ' '.join([word for word in query.split() if word not in stop])\n",
    "query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "query = stem_sentences(query)\n",
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# creating regex pattern object\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "anime_list = find_query(query_list)\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa7bee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the anime numbers from the returned dataframe from the query\n",
    "anime_num_list = []\n",
    "for i in range(len(anime_df)):\n",
    "    for j in range(len(query_anime_df)):\n",
    "        if(anime_df['animeTitle'][i] == query_anime_df['animeTitle'][j]):\n",
    "            anime_num_list.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f60e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the similarity score\n",
    "similarity_score_list = find_similarity(anime_num_list, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fe43f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting popularity and score list\n",
    "animeScore_list, animePopularity_list = find_popularity_and_score_list(anime_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfe2113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the calculated anime scores and popularities\n",
    "cal_animeScore, calanimePopularity = calculate_scores(animeScore_list, animePopularity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52868e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the new score list\n",
    "new_score_list = new_score(similarity_score_list, cal_animeScore, calanimePopularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dd7f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a new column in a dataframe\n",
    "query_anime_df['new_score'] = new_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "609ebd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapsorting them in descending order for the top5 values (if there is)\n",
    "query_anime_df = query_anime_df.sort_values('new_score',ascending= False,kind = 'heapsort').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7c97fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>new_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.137620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.053426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.020647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.013312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "0                                      Dragon Ball Z   \n",
       "2                                    Dragon Ball Kai   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "0  Five years after winning the World Martial Art...   \n",
       "2  Five years after the events of Dragon Ball, ma...   \n",
       "\n",
       "                                                 Url  new_score  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...   0.137620  \n",
       "1  https://myanimelist.net/anime/36946/Dragon_Bal...   0.053426  \n",
       "0  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n   0.020647  \n",
       "2  https://myanimelist.net/anime/6033/Dragon_Ball...   0.013312  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd682c",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c47c5f",
   "metadata": {},
   "source": [
    "**Disclamair**: we took and adapted some of the following coding ideas from https://www.geeksforgeeks.org/k-maximum-sums-non-overlapping-contiguous-sub-arrays/ and also from the discussions on\n",
    "https://www.hackerrank.com/challenges/maximum-subarray-sum/problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae06d9",
   "metadata": {},
   "source": [
    "\n",
    "Consult for managing back-to-back sequences of requests for appointments. A sequence of requests is of the form `[30, 40, 25, 50, 30, 20]` where each number is the time that the person who makes the appointment wants to spend. Aaccept some requests with a break between them. Two consecutive requests are not accepptable. \n",
    "\n",
    "For example, `[30, 50, 20]` is an acceptable solution (of duration 100), but `[30, 40, 50, 20]` is not, because 30 and 40 are two consecutive appointments. \n",
    "\n",
    "**Goal**: provide a schedule that maximizes the total length of the accepted appointments. Provide also:\n",
    "- an algorithm that computes the acceptable solution with the longest possible duration;\n",
    "- a program that given in input an instance in the form given above, gives the optimal solution\n",
    "\n",
    "For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration 110."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692a707",
   "metadata": {},
   "source": [
    "## Formalization of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8abb8e",
   "metadata": {},
   "source": [
    "Given an array of positive integers, find the maximum sum of all the subsequences with the constraint that no two numbers in the subsequences are adjacent in the array and return both the maximum sum and the subsequence(s) that realize the maximum sum. If $f=f(v)$ is the function we want to implement and $v=(30, 40, 25, 50, 30, 20)$, then we should have $f(v)=(40, 50, 20)$ with sum $s=110$, as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ed821",
   "metadata": {},
   "source": [
    "**Algorithmic idea: Dynamic programming**. Given an array $v$, let $v^*[i]$ be the optimal solution using the elements with indices $0,..,i$. In order to have a recursive algorithm that terminates set $v^*[0] = v[0]$, and $\\max(v[0],v[1])=0$, then $v^*[i] = \\max(v^*[i - 1], v^*[i - 2] + v[i])$ for $i = 1, ..., n$ (where $n$ is the dimension of the array given in input). Clearly $v^*[n]$ is the solution we want and it is obteined in $O(n)$. We can then use another array to store which choice is made for each subproblem, and so recover the actual elements chosen.\n",
    "\n",
    "The same idea can be used to solve a more general problem as shown in the examples at the end of this paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e19b1",
   "metadata": {},
   "source": [
    "*Example* . Let $v=(1,2,2,10,1)$ and consider the matrix \\begin{pmatrix} 1 & 0+2=2 & \\dots & 12 & 4 \\\\ 0 & \\max(0,1)=1 & \\dots &3 & 12  \\end{pmatrix}\n",
    "\n",
    "then the maximum subsequence with no adjecent elements sum is 12 and the elements that realize it are (2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f36bc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dec1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows to initialize dictionaries with a lambda function \n",
    "#and provides the default value for a nonexistent key.\n",
    "#so a defaultdict will never raise a KeyError.\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85b95a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(array):\n",
    "    #to track sums\n",
    "    sums = [0]*len(array)\n",
    "    \n",
    "    #to track elements of the input array\n",
    "    #example: if array = [1,2,2,10,1] at the end of the following for loop\n",
    "    #elements = {(0, 1): 1, (0, 2): 2, (1, 2): 3, (2, 10): 12}\n",
    "    elements = defaultdict(lambda: -1)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        #calculate maximum sum \n",
    "        sums[i] = max(sums[i-1], sums[i-2] + array[i])\n",
    "        #memorize\n",
    "        if max(sums[i-1], sums[i-2] + array[i])- (sums[i-2] + array[i]) == 0:\n",
    "            elements[sums[i-2], array[i]] = sums[i]\n",
    "    \n",
    "    #retrieve elements that produce the optimal solution\n",
    "    optimal_subarray = []\n",
    "    \n",
    "    #initialization\n",
    "    max_value = max(elements.values())\n",
    "    \n",
    "    #count is the (first) index containing the max value in the dictionary elements\n",
    "    #for example if elements = {(0, 1): 1, (0, 2): 2, (1, 2): 3, (2, 10): 12} then \n",
    "    #max value is 12 with index 3\n",
    "    count = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "    \n",
    "    #to print the optimal subarray\n",
    "    #example: if elements = {(15, 11): 26} it means that 15 is the cumulative sum\n",
    "    #in this case 15 = 2+5+4+4 and (2,5,4) is the optimal solution, and 11 is the optimal subsequence sum\n",
    "    #the values stored in the second index are those we need, and the first index we use it to check\n",
    "    #when there are no more elements (i.e. count = 0)\n",
    "    while count != 0:\n",
    "        optimal_value = list(elements.keys())[list(elements.values()).index(max_value)][1]\n",
    "        cum_sum = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "        #put an element that realizes the optimal solution to the list\n",
    "        optimal_subarray.insert(0,optimal_value)\n",
    "\n",
    "        max_value = cum_sum\n",
    "        count = cum_sum\n",
    "\n",
    "    \n",
    "    return optimal_subarray, sums[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f534e1",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a84f47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 10], 12)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,2,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87823ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 5, 4], 11)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,3,5,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8968a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([40, 50, 20], 110)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([30, 40, 25, 50, 30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a3b7f",
   "metadata": {},
   "source": [
    "## Solution of a generalization of the previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87eb64",
   "metadata": {},
   "source": [
    "**Attention:** the following code needs refinement. For example it works poorly in some test cases (e.g. when in the array there are duplicate elements or a lot of contiguous elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aacdd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(lambda: -1)\n",
    "prefix_sum = []\n",
    "trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eac913c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array_sum(i, j):\n",
    "    \"\"\"\n",
    "    Input: indexes i,j of an array v with i<j\n",
    "    Output: v[i]+v[i+1]+...+v[j-1]+v[j]\n",
    "    Remark: if i>j returns 0\n",
    "    \"\"\"\n",
    "    if i == 0:\n",
    "        return prefix_sum[j]\n",
    "    return (prefix_sum[j] - prefix_sum[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6ca8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_sum(cur, v, k):\n",
    "    \"\"\"\n",
    "    Input: current element cur, array v, positive integer k \n",
    "    Output: current maximum sum \n",
    "    Remark: this function allows also to track the elements that realise the maximum sum.      \n",
    "    \"\"\"\n",
    "    if cur >= len(v):\n",
    "        return 0\n",
    "    if dd[cur] != -1:\n",
    "        return dd[cur]\n",
    "    \n",
    "    #use the following line when all the elements in the array are positive, \n",
    "    #else set s1 and s2 to -Infinity\n",
    "    s1 = -1; s2 = -1\n",
    "    \n",
    "    #choose subarray starting at the current element \"cur\"\n",
    "    if cur + k - 1 < len(v):\n",
    "        # Remark: sub_array_sum(cur,cur)=0\n",
    "        s1 = sub_array_sum(cur, cur + k - 1) + maximum_sum(cur + k + 1, v, k)\n",
    "    \n",
    "    #ignore subarray starting at \"cur\"\n",
    "    s2 = maximum_sum(cur + 1, v, k)\n",
    "    dd[cur] = max(s1, s2)\n",
    "    \n",
    "    if s1 >= s2:\n",
    "        #keep track of the elements that realise the maximum sum\n",
    "        trace[cur] = (True, cur + k + 1)\n",
    "        return s1\n",
    "    trace[cur] = (False, cur + 1)\n",
    "    \n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb106191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array(v, trace, k):\n",
    "    \"\"\"\n",
    "    Input: array v, array trace, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray\n",
    "    Remark: this function allows to return non-consecutive subarrays of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    subArrays = []\n",
    "    for i in range(len(trace)):\n",
    "        if trace[i][0]:\n",
    "            subArrays.append(v[i : i + k])\n",
    "        i = trace[i][1]\n",
    "\n",
    "    return subArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cc94b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_solution(v, k):\n",
    "    \"\"\"\n",
    "    Input: array v, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray(s)\n",
    "    Remark: this function allows to return non-consecutive optimal subarray(s) of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    global dd, trace, prefix_sum\n",
    "    dd = defaultdict(lambda: -1)\n",
    "    \n",
    "    #initialization\n",
    "    trace = [(False, 0)] * len(v)\n",
    "    prefix_sum = [0] * len(v)\n",
    "    prefix_sum[0] = v[0]\n",
    "    \n",
    "    for i in range(1,len(v)):\n",
    "        prefix_sum[i] += prefix_sum[i - 1] + v[i]\n",
    "        \n",
    "    print(\"Array :\", v)\n",
    "    print(\"Max sum: \", maximum_sum(0, v, k))\n",
    "    print(\"Subarrays: \", sub_array(v, trace, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65bd5d",
   "metadata": {},
   "source": [
    "## Some examples of solution of a more general problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b224df3",
   "metadata": {},
   "source": [
    "To sole a generalized version of the problem take $k>1$, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48e97123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  9\n",
      "Subarrays:  [[1], [3], [5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab820c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[1, 2], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "026c57c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f3347",
   "metadata": {},
   "source": [
    "## Alternative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3f428",
   "metadata": {},
   "source": [
    "With immense surprise we have found that it is possible to solve the problem with just 3 lines of code! See https://codegolf.stackexchange.com/questions/183390/maximum-summed-subsequences-with-non-adjacent-items?answertab=active#tab-top for more deatils. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe2f97",
   "metadata": {},
   "source": [
    "Here it is the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90dd15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [30, 40, 25, 50, 30, 20]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6479130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 50, 20] 110\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c2ec0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 2, 3, 5, 4]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2539d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4] 8\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef307e",
   "metadata": {},
   "source": [
    "**Credits**: Chas Brown https://codegolf.stackexchange.com/users/69880/chas-brown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6192e2",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: Some parts of the following code was inspired by looking at the work that was done last year about https://www.goodreads.com, for example by https://github.com/GiorgiaSalvatori/ADM-HW3/blob/main/main.ipynb. Also the following post was useful https://towardsdatascience.com/how-to-use-selenium-to-web-scrape-with-example-80f9b23a843a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "Our goal is to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList. There is no provided dataset, so we create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8756d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to install\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm # useful for progress bars\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import codecs\n",
    "import lxml\n",
    "import time\n",
    "import re\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "We start from the list of animes to include in the corpus of documents the search engine will work on. In particular, we focus on the top animes ever list: https://myanimelist.net/topanime.php.  The list is long and splitted in many pages. The first thing we will do is to retrieve the urls (and the names) of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list. At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `selenium` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debbf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9204e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service('/Users/dany/Desktop/adm-hw3/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium with Chrome\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12aa4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with links of each anime\n",
    "df = pd.DataFrame(columns = ['Href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f107bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [07:12<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# go page by page and and store links in a list\n",
    "anime_list = []\n",
    "\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime_list.append((link.contents[0], link.get('href')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7dbe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19125\n"
     ]
    }
   ],
   "source": [
    "# total number of animes\n",
    "print(len(anime_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "442da9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19124"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicates: ok no duplicates\n",
    "df['Href'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38884487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign list to dataframe\n",
    "df['Href'] = [item[1] for item in anime_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94629a2b",
   "metadata": {},
   "source": [
    "The following code shows the first informations we have acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c0bbbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://myanimelist.net/anime/5114/Fullmetal_A...\n",
       "1         https://myanimelist.net/anime/28977/Gintama°\n",
       "2    https://myanimelist.net/anime/38524/Shingeki_n...\n",
       "3       https://myanimelist.net/anime/9253/Steins_Gate\n",
       "4    https://myanimelist.net/anime/42938/Fruits_Bas...\n",
       "Name: Href, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Href'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faf5a9",
   "metadata": {},
   "source": [
    "We save the dataframe into a *csv* file without header and comma separator. This is equivalent to a *txt* file, with not only the urls, but also the names of the animes that may be of help in some data processing stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09ed84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('urls.csv', sep = ' ', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc99b2",
   "metadata": {},
   "source": [
    "We could also create a dictionary as this is useful in some circumnstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b57c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood', 'https://myanimelist.net/anime/28977/Gintama°', 'https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2', 'https://myanimelist.net/anime/9253/Steins_Gate', 'https://myanimelist.net/anime/42938/Fruits_Basket__The_Final']\n"
     ]
    }
   ],
   "source": [
    "#keys\n",
    "name = []   \n",
    "#values\n",
    "url = []    \n",
    "\n",
    "for item in anime_list:\n",
    "    name.append(item[0])\n",
    "    url.append(item[1])\n",
    "    \n",
    "D = dict(zip(name, url))\n",
    "\n",
    "# display first 5 urls\n",
    "print(list(D.values())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d319220",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c96f65",
   "metadata": {},
   "source": [
    "At this point we have all the html documents about the animes of interest and we can start to extract the animes informations:\n",
    "- Anime Name (to save as `animeTitle`): String\n",
    "-Anime Type (to save as `animeType`): String\n",
    "-Number of episode (to save as `animeNumEpisode`): Integer\n",
    "-Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "-Number of members (to save as `animeNumMembers`): Integer\n",
    "-Score (to save as `animeScore`): Float\n",
    "-Users (to save as `animeUsers`): Integer\n",
    "-Rank (to save as `animeRank`): Integer\n",
    "-Popularity (to save as `animePopularity`): Integer\n",
    "-Synopsis (to save as `animeDescription`): String\n",
    "-Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "-Characters (to save as `animeCharacters`): List of strings.\n",
    "-Voices (to save as `animeVoices`): List of strings\n",
    "-Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48fb6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "\n",
    "\n",
    "# I still don't know how to integrate directory paths\n",
    "# The following code should work only with files\n",
    "# For the sake of simplicity the following code works for the first 5 files\n",
    "# article_0, ...., article_5\n",
    "# Attention: There are some bugs \n",
    "for i in range(5):\n",
    "    # anime titles as test case above\n",
    "    # a similar code can be used for the other informations\n",
    "    path = \"article_\" + str(i) + \".html\"\n",
    "    file = codecs.open(path, \"r\", \"utf-8\")\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    animeTitle.append(soup.find_all('strong')[0].contents[0])\n",
    "    \n",
    "    # left of the html page\n",
    "    # e.g. for article_0 under Alternative Titles we have English: Fullmetal Alchemist: Brotherhood \n",
    "    # that corresponds to \n",
    "    # <div class=\"spaceit_pad\"> Fullmetal Alchemist: Brotherhood <span class=\"dark_text\">English:</span></div>\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            # TYPES\n",
    "            if span.contents[0] == 'Type:':\n",
    "                animeType.append(div.find_all('a')[0].contents[0])\n",
    "            # NUMBER OF EPISODES\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                animeNumEpisode.append(int(div.contents[2]))\n",
    "            # DATES\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                if len(div.contents[2]) > 21:\n",
    "                    releaseDate.append(pd.to_datetime(div.contents[2][1:16]))\n",
    "                    endDate.append(pd.to_datetime(div.contents[2][18:-3]))\n",
    "                else:\n",
    "                    releaseDate.append(pd.to_datetime(div.contents[2][4:-4]))\n",
    "                    endDate.append('-')\n",
    "            #if span.contents[0] == 'Aired:':\n",
    "                #dates = divs[i].contents[0].strip().split(\"to\")\n",
    "                #releaseDate = datetime.strptime(dates[0].strip(), '%b %d, %Y').date()\n",
    "                #endDate = datetime.strptime(dates[1].strip(), '%b %d, %Y').date()\n",
    "                    \n",
    "    # center of the html page\n",
    "    # similar to what was done before\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        \n",
    "        # MEMBERS\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        \n",
    "        \n",
    "        # SCORE\n",
    "        rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "        animeScore.append(float(rating.text.strip()))\n",
    "    \n",
    "        # USERS\n",
    "        users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "        # here we we eliminate the word 'user '   \n",
    "        # that is why there is the [:-6] part\n",
    "        # we also replace the comma divisor\n",
    "        animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        \n",
    "        \n",
    "        # RANK\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "       \n",
    "    \n",
    "        # POPULARITY\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # DESCRIPTION\n",
    "    # center of the html page\n",
    "    animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "     \n",
    "    \n",
    "    # RELATED \n",
    "    # center of the html page\n",
    "    x = []\n",
    "    y = []\n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    for tr in related:\n",
    "        td = tr.find_all(\"td\")\n",
    "        for i in range(0, len(td), 2):\n",
    "            x.append(td[i].contents[0])\n",
    "            t = td[i+1].find_all(\"a\")\n",
    "            y.append(t[0].contents[0])\n",
    "        animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "    \n",
    "    \n",
    "    # CHARACTERS\n",
    "    # center of the html page (bottom\n",
    "    characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "    x = []\n",
    "    for i in chars:\n",
    "        x.append(i.contents[0].contents[0])\n",
    "    animeCharacters.append(x)\n",
    "    \n",
    "    \n",
    "    # VOICES\n",
    "    # center of the html page (bottom)\n",
    "    voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "    y = []\n",
    "    for i in voices:\n",
    "        y.append(i.contents[1].contents[0])\n",
    "    animeVoices.append(y)\n",
    "\n",
    "    \n",
    "    # STAFF\n",
    "    # center of the html page (bottom)\n",
    "    staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    staff = staff[1].find_all(\"td\")\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(1, len(staff), 2):\n",
    "        x.append(staff[i].contents[1].contents[0])\n",
    "        y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "    animeStaff.append([list(i) for i in list(zip(x,y))])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7da2325a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Episodes</th>\n",
       "      <th>Release date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Members</th>\n",
       "      <th>Score</th>\n",
       "      <th>Users</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Description</th>\n",
       "      <th>Related</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Voices</th>\n",
       "      <th>Staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>TV</td>\n",
       "      <td>64</td>\n",
       "      <td>2009-04-05</td>\n",
       "      <td>2010-07-04</td>\n",
       "      <td>2675751</td>\n",
       "      <td>9.16</td>\n",
       "      <td>1622384</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>[Adaptation: Fullmetal Alchemist, Alternative ...</td>\n",
       "      <td>[Elric, Edward, Elric, Alphonse, Mustang, Roy,...</td>\n",
       "      <td>[Park, Romi, Kugimiya, Rie, Miki, Shinichiro, ...</td>\n",
       "      <td>[[Cook, Justin, Producer], [Yonai, Noritomo, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gintama°</td>\n",
       "      <td>TV</td>\n",
       "      <td>51</td>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>483807</td>\n",
       "      <td>9.09</td>\n",
       "      <td>169476</td>\n",
       "      <td>2</td>\n",
       "      <td>337</td>\n",
       "      <td>u</td>\n",
       "      <td>[Adaptation: Gintama, Prequel: Gintama Movie 2...</td>\n",
       "      <td>[Sakata, Gintoki, Kagura, Shimura, Shinpachi, ...</td>\n",
       "      <td>[Sugita, Tomokazu, Kugimiya, Rie, Sakaguchi, D...</td>\n",
       "      <td>[[Fujita, Youichi, Director, Storyboard, Plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shingeki no Kyojin Season 3 Part 2</td>\n",
       "      <td>TV</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>1596039</td>\n",
       "      <td>9.09</td>\n",
       "      <td>1087519</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>n</td>\n",
       "      <td>[Adaptation: Shingeki no Kyojin, Prequel: Shin...</td>\n",
       "      <td>[Levi, Yeager, Eren, Ackerman, Mikasa, Arlert,...</td>\n",
       "      <td>[Kamiya, Hiroshi, Kaji, Yuki, Ishikawa, Yui, I...</td>\n",
       "      <td>[[Yabuta, Shuuhei, Producer], [Wada, Jouji, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>TV</td>\n",
       "      <td>24</td>\n",
       "      <td>2011-04-06</td>\n",
       "      <td>2011-09-14</td>\n",
       "      <td>2090910</td>\n",
       "      <td>9.09</td>\n",
       "      <td>1109700</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>d</td>\n",
       "      <td>[Adaptation: Steins;Gate, Alternative setting:...</td>\n",
       "      <td>[Okabe, Rintarou, Makise, Kurisu, Shiina, Mayu...</td>\n",
       "      <td>[Miyano, Mamoru, Imai, Asami, Hanazawa, Kana, ...</td>\n",
       "      <td>[[Iwasa, Gaku, Producer], [Yasuda, Takeshi, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fruits Basket: The Final</td>\n",
       "      <td>TV</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>275214</td>\n",
       "      <td>9.07</td>\n",
       "      <td>113310</td>\n",
       "      <td>5</td>\n",
       "      <td>651</td>\n",
       "      <td>r</td>\n",
       "      <td>[Adaptation: Fruits Basket, Prequel: Fruits Ba...</td>\n",
       "      <td>[Souma, Kyou, Honda, Tooru, Souma, Yuki, Souma...</td>\n",
       "      <td>[Uchida, Yuuma, Iwami, Manaka, Shimazaki, Nobu...</td>\n",
       "      <td>[[Ibata, Yoshihide, Director], [Aketagawa, Jin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title Type Episodes Release date   End date  \\\n",
       "0    Fullmetal Alchemist: Brotherhood   TV       64   2009-04-05 2010-07-04   \n",
       "1                            Gintama°   TV       51   2015-04-08 2016-03-30   \n",
       "2  Shingeki no Kyojin Season 3 Part 2   TV       10   2019-04-29 2019-07-01   \n",
       "3                         Steins;Gate   TV       24   2011-04-06 2011-09-14   \n",
       "4            Fruits Basket: The Final   TV       13   2021-04-06 2021-06-29   \n",
       "\n",
       "   Members Score    Users Rank Popularity Description  \\\n",
       "0  2675751  9.16  1622384    1          3           H   \n",
       "1   483807  9.09   169476    2        337           u   \n",
       "2  1596039  9.09  1087519    3         33           n   \n",
       "3  2090910  9.09  1109700    4         11           d   \n",
       "4   275214  9.07   113310    5        651           r   \n",
       "\n",
       "                                             Related  \\\n",
       "0  [Adaptation: Fullmetal Alchemist, Alternative ...   \n",
       "1  [Adaptation: Gintama, Prequel: Gintama Movie 2...   \n",
       "2  [Adaptation: Shingeki no Kyojin, Prequel: Shin...   \n",
       "3  [Adaptation: Steins;Gate, Alternative setting:...   \n",
       "4  [Adaptation: Fruits Basket, Prequel: Fruits Ba...   \n",
       "\n",
       "                                          Characters  \\\n",
       "0  [Elric, Edward, Elric, Alphonse, Mustang, Roy,...   \n",
       "1  [Sakata, Gintoki, Kagura, Shimura, Shinpachi, ...   \n",
       "2  [Levi, Yeager, Eren, Ackerman, Mikasa, Arlert,...   \n",
       "3  [Okabe, Rintarou, Makise, Kurisu, Shiina, Mayu...   \n",
       "4  [Souma, Kyou, Honda, Tooru, Souma, Yuki, Souma...   \n",
       "\n",
       "                                              Voices  \\\n",
       "0  [Park, Romi, Kugimiya, Rie, Miki, Shinichiro, ...   \n",
       "1  [Sugita, Tomokazu, Kugimiya, Rie, Sakaguchi, D...   \n",
       "2  [Kamiya, Hiroshi, Kaji, Yuki, Ishikawa, Yui, I...   \n",
       "3  [Miyano, Mamoru, Imai, Asami, Hanazawa, Kana, ...   \n",
       "4  [Uchida, Yuuma, Iwami, Manaka, Shimazaki, Nobu...   \n",
       "\n",
       "                                               Staff  \n",
       "0  [[Cook, Justin, Producer], [Yonai, Noritomo, P...  \n",
       "1  [[Fujita, Youichi, Director, Storyboard, Plann...  \n",
       "2  [[Yabuta, Shuuhei, Producer], [Wada, Jouji, Pr...  \n",
       "3  [[Iwasa, Gaku, Producer], [Yasuda, Takeshi, Pr...  \n",
       "4  [[Ibata, Yoshihide, Director], [Aketagawa, Jin...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(\n",
    "    [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, \n",
    "     animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \n",
    "     animeCharacters, animeVoices, animeStaff], \n",
    "    index=['Title', 'Type', 'Episodes','Release date', 'End date', 'Members', 'Score', \n",
    "           'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff']).T\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52b3a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row create a tsv file\n",
    "for i in range(5):\n",
    "    with open('anime_'+str(i)+'.tsv', 'wt') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow(x for x in df.iloc[i]) #the value under each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd88008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961689af",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: Some parts of the following code was inspired by looking at the work that was done last year about https://www.goodreads.com, for example by https://github.com/GiorgiaSalvatori/ADM-HW3/blob/main/main.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "Our goal is to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList https://myanimelist.net. There is no provided dataset, so we create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import codecs\n",
    "import csv\n",
    "import os \n",
    "\n",
    "#import re\n",
    "#import lxml\n",
    "#import time\n",
    "#import datetime\n",
    "#from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "We start from the list of animes to include in the corpus of documents the search engine will work on. In particular, we focus on the top animes ever list: https://myanimelist.net/topanime.php.  The list is long and splitted in many pages. The first thing we will do is to retrieve the urls (and the names) of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list. At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `BeautifulSoup` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE links.txt EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (some minutes)\n",
    "\n",
    "# open an empty .txt file to store the urls we need\n",
    "links_text = open(\"links.txt\", \"w\")\n",
    "\n",
    "# go page by page in the site and scrap the urls we need\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                data = link.get('href')\n",
    "                # write the scrapped urls in the .txt file with '\\n' at the end of each raw\n",
    "                links_text.write(data)\n",
    "                links_text.write(\"\\n\")\n",
    "\n",
    "# close the .txt file\n",
    "links_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5089af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE IF AND ONLY IF THE links.txt FILE HAS BEEN CREATED\n",
    "\n",
    "# Read the number of lines in the .txt file\n",
    "file = open(\"links.txt\", \"r\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print('There are total {} lines in this file.'.format(line_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY TREE ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite some time (>25 hours)\n",
    "# REMARK: there is an issue with high frequency site-connections that blocks most of the page requests \n",
    "# a time delay between page requests has been included to solve that issue\n",
    "\n",
    "\n",
    "file = open(\"links.txt\", \"r\")\n",
    "lines = file.read().split('\\n')\n",
    "file.close()\n",
    "# returns current working directory\n",
    "base = os.getcwd()  \n",
    "# initialize the number of the first directory to be created\n",
    "t = 0\n",
    "# we use the previously created list of lines to get the urls we need\n",
    "scrapped_urls = lines[0:-1]\n",
    "for i in range(len(scrapped_urls)):\n",
    "    if(i%50==0):\n",
    "        # create a new folder\n",
    "        # remark: the newley created pages will start from 0\n",
    "        page_identifier = i-(49*t)\n",
    "        # subdirectory \n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "        # parent directories\n",
    "        parent_dir = base\n",
    "        # path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        # make directory\n",
    "        os.makedirs(path)\n",
    "        # check\n",
    "        # print(\"Directory '%s' created\" %directory)\n",
    "        # change directory \n",
    "        os.chdir(path)\n",
    "        t += 1\n",
    "\n",
    "    # to avoid the issue with high frequency site-connections  \n",
    "    time.sleep(5)   \n",
    "\n",
    "    # get urls\n",
    "    URL = scrapped_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    # parsing\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # saving\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "        \n",
    "    # check\n",
    "    # print(f\"Article {i} chas been created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbdc2e",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ab9ff",
   "metadata": {},
   "source": [
    "At this point we have all the html documents about the animes of interest and we can start to extract the animes informations:\n",
    "- Anime Name (to save as `animeTitle`): String\n",
    "- Anime Type (to save as `animeType`): String\n",
    "- Number of episode (to save as `animeNumEpisode`): Integer\n",
    "- Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "- Number of members (to save as `animeNumMembers`): Integer\n",
    "- Score (to save as `animeScore`): Float\n",
    "- Users (to save as `animeUsers`): Integer\n",
    "- Rank (to save as `animeRank`): Integer\n",
    "- Popularity (to save as `animePopularity`): Integer\n",
    "- Synopsis (to save as `animeDescription`): String\n",
    "- Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "- Characters (to save as `animeCharacters`): List of strings.\n",
    "- Voices (to save as `animeVoices`): List of strings\n",
    "- Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace1498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE (WITH A SUBSET OF PAGES)\n",
    "\n",
    "# REMARK: the execution with 10 pages should take a couple of minutes, and \n",
    "# the execution with 100 pages should take more or less 10 minutes\n",
    "\n",
    "# REMARK: page_382 in the html_pages directory has 22 articles\n",
    "# this means that when the outer for loop is executed with j == 382\n",
    "# the inner for loop should span only the values i == 0,1,2,...,20,21\n",
    "\n",
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "\n",
    "\n",
    "for j in range(382):\n",
    "    for i in range(50):\n",
    "        # path depends on the directory tree that was previously created\n",
    "        path = \"html_pages/\"+ \"page_\"+str(j)+\".html/article_\" + str(50*j+i) + \".html\"\n",
    "\n",
    "        file = codecs.open(path, \"r\", \"utf-8\")\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "        #TITLE\n",
    "        try:\n",
    "            animeTitle.append(soup.find_all('strong')[0].contents[0])\n",
    "        except: \n",
    "            animeTitle.append('NA')\n",
    "        \n",
    "        # DATES\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                try:\n",
    "                    if len(div.contents[2]) > 21:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        end = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        endDate.append(end)\n",
    "                    else:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        endDate.append('-')\n",
    "                except:\n",
    "                        releaseDate.append('NA')\n",
    "                        endDate.append('NA')\n",
    "        \n",
    "        divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "        for div in divs:\n",
    "            spans = div.find_all(\"span\")\n",
    "            for span in spans:\n",
    "\n",
    "                # TYPES\n",
    "                try:\n",
    "                    if span.contents[0] == 'Type:':\n",
    "                        animeType.append(div.find_all('a')[0].contents[0])\n",
    "                except:\n",
    "                    animeType.append('NA')\n",
    "\n",
    "                # NUMBER OF EPISODES\n",
    "                if span.contents[0] == 'Episodes:':\n",
    "                    try: \n",
    "                        animeNumEpisode.append(int(div.contents[2]))\n",
    "                    except:\n",
    "                        animeNumEpisode.append(0)\n",
    "\n",
    "\n",
    "        divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "        for div in divs:\n",
    "\n",
    "            # MEMBERS\n",
    "            try:\n",
    "                members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "                animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "            except: \n",
    "                animeNumMembers.append(0)\n",
    "\n",
    "            # USERS\n",
    "            users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "            # here we we eliminate the word 'user '   \n",
    "            # that is why there is the [:-6] part\n",
    "            # we also replace the comma divisor\n",
    "            try:\n",
    "                animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "            except:\n",
    "                animeUsers.append(0)\n",
    "\n",
    "            # SCORE\n",
    "            rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "            try:        \n",
    "                animeScore.append(float(rating.text.strip()))\n",
    "            except:\n",
    "                animeScore.append(None)\n",
    "\n",
    "            # RANK\n",
    "            try:\n",
    "                rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "                animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "            except:\n",
    "                animeRank.append(None)\n",
    "\n",
    "            # POPULARITY\n",
    "            try:   \n",
    "                popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "                animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "            except:\n",
    "                animePopularity.append(None)\n",
    "\n",
    "        # DESCRIPTION\n",
    "        try:\n",
    "            description = soup.find_all(\"p\", {\"itemprop\": \"description\"})\n",
    "            for br in description[0].find_all(\"br\"):\n",
    "                br.replace_with(\"\\n\")\n",
    "            animeDescription.append(description[0].contents)\n",
    "        except: \n",
    "            animeDescription.append('NA')\n",
    "        \n",
    "        \n",
    "        # RELATED \n",
    "        try:\n",
    "            related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "            x = []\n",
    "            y = []\n",
    "            for tr in related:\n",
    "                td = tr.find_all(\"td\")\n",
    "                for i in range(0, len(td), 2):\n",
    "                    x.append(td[i].contents[0])\n",
    "                    try:\n",
    "                        t = td[i+1].find_all(\"a\")\n",
    "                        y.append(t[0].contents[0])\n",
    "                    except:\n",
    "                        y.append('NA')\n",
    "\n",
    "                animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "        except: \n",
    "            animeRelated.append('NA')\n",
    "\n",
    "        # CHARACTERS\n",
    "        try:\n",
    "            characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "            x = []\n",
    "            for i in chars:\n",
    "                x.append(i.contents[0].contents[0])\n",
    "            animeCharacters.append(x)\n",
    "        except:\n",
    "            animeCharacters.append(\"NA\")\n",
    "\n",
    "        # VOICES\n",
    "        try:\n",
    "            voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "            y = []\n",
    "            for i in voices:\n",
    "                y.append(i.contents[1].contents[0])\n",
    "            animeVoices.append(y)\n",
    "        except:\n",
    "            animeVoices.append(\"NA\")\n",
    "\n",
    "        # STAFF\n",
    "        try:\n",
    "            staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            staff = staff[1].find_all(\"td\")\n",
    "            x = []\n",
    "            y = []\n",
    "            for i in range(1, len(staff), 2):\n",
    "                x.append(staff[i].contents[1].contents[0])\n",
    "                y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "            animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "        except:\n",
    "            animeStaff.append(\"NA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2340e5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(\n",
    "    [animeTitle, animeType, animeNumEpisode, animeNumMembers, \n",
    "     animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \n",
    "     animeCharacters, animeVoices, animeStaff], \n",
    "    index=['Title', 'Type', 'Episodes', 'Members', 'Score', \n",
    "           'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16301ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Episodes</th>\n",
       "      <th>Members</th>\n",
       "      <th>Score</th>\n",
       "      <th>Users</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Description</th>\n",
       "      <th>Related</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Voices</th>\n",
       "      <th>Staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>TV</td>\n",
       "      <td>64</td>\n",
       "      <td>2675751</td>\n",
       "      <td>9.16</td>\n",
       "      <td>1622384</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[After a horrific alchemy experiment goes wron...</td>\n",
       "      <td>[Adaptation: Fullmetal Alchemist, Alternative ...</td>\n",
       "      <td>[Elric, Edward, Elric, Alphonse, Mustang, Roy,...</td>\n",
       "      <td>[Park, Romi, Kugimiya, Rie, Miki, Shinichiro, ...</td>\n",
       "      <td>[[Cook, Justin, Producer], [Yonai, Noritomo, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gintama°</td>\n",
       "      <td>TV</td>\n",
       "      <td>51</td>\n",
       "      <td>483807</td>\n",
       "      <td>9.09</td>\n",
       "      <td>169476</td>\n",
       "      <td>2</td>\n",
       "      <td>337</td>\n",
       "      <td>[Gintoki, Shinpachi, and Kagura return as the ...</td>\n",
       "      <td>[Adaptation: Gintama, Prequel: Gintama Movie 2...</td>\n",
       "      <td>[Sakata, Gintoki, Kagura, Shimura, Shinpachi, ...</td>\n",
       "      <td>[Sugita, Tomokazu, Kugimiya, Rie, Sakaguchi, D...</td>\n",
       "      <td>[[Fujita, Youichi, Director, Storyboard, Plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shingeki no Kyojin Season 3 Part 2</td>\n",
       "      <td>TV</td>\n",
       "      <td>10</td>\n",
       "      <td>1596039</td>\n",
       "      <td>9.09</td>\n",
       "      <td>1087519</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>[Seeking to restore humanity's diminishing hop...</td>\n",
       "      <td>[Adaptation: Shingeki no Kyojin, Prequel: Shin...</td>\n",
       "      <td>[Levi, Yeager, Eren, Ackerman, Mikasa, Arlert,...</td>\n",
       "      <td>[Kamiya, Hiroshi, Kaji, Yuki, Ishikawa, Yui, I...</td>\n",
       "      <td>[[Yabuta, Shuuhei, Producer], [Wada, Jouji, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>TV</td>\n",
       "      <td>24</td>\n",
       "      <td>2090910</td>\n",
       "      <td>9.09</td>\n",
       "      <td>1109700</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[The self-proclaimed mad scientist Rintarou Ok...</td>\n",
       "      <td>[Adaptation: Steins;Gate, Alternative setting:...</td>\n",
       "      <td>[Okabe, Rintarou, Makise, Kurisu, Shiina, Mayu...</td>\n",
       "      <td>[Miyano, Mamoru, Imai, Asami, Hanazawa, Kana, ...</td>\n",
       "      <td>[[Iwasa, Gaku, Producer], [Yasuda, Takeshi, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fruits Basket: The Final</td>\n",
       "      <td>TV</td>\n",
       "      <td>13</td>\n",
       "      <td>275214</td>\n",
       "      <td>9.07</td>\n",
       "      <td>113310</td>\n",
       "      <td>5</td>\n",
       "      <td>651</td>\n",
       "      <td>[Hundreds of years ago, the Chinese Zodiac spi...</td>\n",
       "      <td>[Adaptation: Fruits Basket, Prequel: Fruits Ba...</td>\n",
       "      <td>[Souma, Kyou, Honda, Tooru, Souma, Yuki, Souma...</td>\n",
       "      <td>[Uchida, Yuuma, Iwami, Manaka, Shimazaki, Nobu...</td>\n",
       "      <td>[[Ibata, Yoshihide, Director], [Aketagawa, Jin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title Type Episodes  Members Score    Users  \\\n",
       "0    Fullmetal Alchemist: Brotherhood   TV       64  2675751  9.16  1622384   \n",
       "1                            Gintama°   TV       51   483807  9.09   169476   \n",
       "2  Shingeki no Kyojin Season 3 Part 2   TV       10  1596039  9.09  1087519   \n",
       "3                         Steins;Gate   TV       24  2090910  9.09  1109700   \n",
       "4            Fruits Basket: The Final   TV       13   275214  9.07   113310   \n",
       "\n",
       "  Rank Popularity                                        Description  \\\n",
       "0    1          3  [After a horrific alchemy experiment goes wron...   \n",
       "1    2        337  [Gintoki, Shinpachi, and Kagura return as the ...   \n",
       "2    3         33  [Seeking to restore humanity's diminishing hop...   \n",
       "3    4         11  [The self-proclaimed mad scientist Rintarou Ok...   \n",
       "4    5        651  [Hundreds of years ago, the Chinese Zodiac spi...   \n",
       "\n",
       "                                             Related  \\\n",
       "0  [Adaptation: Fullmetal Alchemist, Alternative ...   \n",
       "1  [Adaptation: Gintama, Prequel: Gintama Movie 2...   \n",
       "2  [Adaptation: Shingeki no Kyojin, Prequel: Shin...   \n",
       "3  [Adaptation: Steins;Gate, Alternative setting:...   \n",
       "4  [Adaptation: Fruits Basket, Prequel: Fruits Ba...   \n",
       "\n",
       "                                          Characters  \\\n",
       "0  [Elric, Edward, Elric, Alphonse, Mustang, Roy,...   \n",
       "1  [Sakata, Gintoki, Kagura, Shimura, Shinpachi, ...   \n",
       "2  [Levi, Yeager, Eren, Ackerman, Mikasa, Arlert,...   \n",
       "3  [Okabe, Rintarou, Makise, Kurisu, Shiina, Mayu...   \n",
       "4  [Souma, Kyou, Honda, Tooru, Souma, Yuki, Souma...   \n",
       "\n",
       "                                              Voices  \\\n",
       "0  [Park, Romi, Kugimiya, Rie, Miki, Shinichiro, ...   \n",
       "1  [Sugita, Tomokazu, Kugimiya, Rie, Sakaguchi, D...   \n",
       "2  [Kamiya, Hiroshi, Kaji, Yuki, Ishikawa, Yui, I...   \n",
       "3  [Miyano, Mamoru, Imai, Asami, Hanazawa, Kana, ...   \n",
       "4  [Uchida, Yuuma, Iwami, Manaka, Shimazaki, Nobu...   \n",
       "\n",
       "                                               Staff  \n",
       "0  [[Cook, Justin, Producer], [Yonai, Noritomo, P...  \n",
       "1  [[Fujita, Youichi, Director, Storyboard, Plann...  \n",
       "2  [[Yabuta, Shuuhei, Producer], [Wada, Jouji, Pr...  \n",
       "3  [[Iwasa, Gaku, Producer], [Yasuda, Takeshi, Pr...  \n",
       "4  [[Ibata, Yoshihide, Director], [Aketagawa, Jin...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893d6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY TREE ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution should take a few seconds\n",
    "\n",
    "# create a directory tree for .tsv files\n",
    "os.mkdir('tsv_files')\n",
    "for j in range(382):\n",
    "    os.mkdir('tsv_files/'+'files_'+str(j)+ '.tsv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7762c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE WITH THE SAME SUBSET OF PAGES USED IN THE PREVIOUS CELLS\n",
    "# IF THE .tsv FILES ALREADY EXIST THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution should take a few seconds\n",
    "\n",
    "# create a .tsv file for each anime and put it in the corresponding directory \n",
    "for j in range(382):\n",
    "    for i in range(50):\n",
    "        with open('tsv_files/'+'files_'+str(j)+ '.tsv/'+'anime_'+str(50*j+i)+'.tsv', 'w') as file:\n",
    "            tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "            # header\n",
    "            tsv_writer.writerow([x for x in dataset.columns]) \n",
    "            # columns\n",
    "            tsv_writer.writerow(x for x in dataset.iloc[i+50*j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9ca39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

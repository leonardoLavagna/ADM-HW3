{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bc6156",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d4bc9",
   "metadata": {},
   "source": [
    "Our goal is to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList https://myanimelist.net. There is no provided dataset, so we create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import codecs\n",
    "import csv\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fc270",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961689af",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: Some parts of the following code was inspired by looking at the work that was done last year about https://www.goodreads.com, for example by https://github.com/GiorgiaSalvatori/ADM-HW3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b994d",
   "metadata": {},
   "source": [
    "We start from the list of animes to include in the corpus of documents the search engine will work on. In particular, we focus on the top animes ever list: https://myanimelist.net/topanime.php.  The list is long and splitted in many pages. The first thing we will do is to retrieve the urls (and the names) of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770733b",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be20482",
   "metadata": {},
   "source": [
    "Here we will extract the *urls* and the *names* of the animes in the list. At first we can have an idea of the necessary steps to extract the informations we want by working on a single anime in the list and then proceed by iteration. \n",
    "\n",
    "After inspecting the HTML code of the site, we saw that the all the informations we need from a single anime are stored in  `tr` blocks inside a single `table` that contains the list of all the top animes in the site. To get the  name of an anime in the list we should work on `a` tags, whereas to get the url we need to work on `td` tags (leveraging the property `href`). \n",
    "\n",
    "Knowing these HTML details we can use the `BeautifulSoup` library to do the web-scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE FILE links.txt EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (some minutes)\n",
    "\n",
    "# open an empty .txt file to store the urls we need\n",
    "links_text = open(\"links.txt\", \"w\")\n",
    "\n",
    "# go page by page in the site and scrap the urls we need\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                data = link.get('href')\n",
    "                # write the scrapped urls in the .txt file with '\\n' at the end of each raw\n",
    "                links_text.write(data)\n",
    "                links_text.write(\"\\n\")\n",
    "\n",
    "# close the .txt file\n",
    "links_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5089af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE IF AND ONLY IF THE links.txt FILE HAS BEEN CREATED\n",
    "\n",
    "# Read the number of lines in the .txt file\n",
    "file = open(\"links.txt\", \"r\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print('There are total {} lines in this file.'.format(line_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f09ed",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39d3",
   "metadata": {},
   "source": [
    "We procede to:\n",
    "- download the html corresponding to each of the collected urls;\n",
    "- save its html in a file;\n",
    "- organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n",
    "\n",
    "To do so we extensively use the `os` library to create directories, changing paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY TREE ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite some time (>25 hours)\n",
    "# REMARK: there is an issue with high frequency site-connections that blocks most of the page requests \n",
    "# a time delay between page requests has been included to solve that issue\n",
    "\n",
    "\n",
    "file = open(\"links.txt\", \"r\")\n",
    "lines = file.read().split('\\n')\n",
    "file.close()\n",
    "# returns current working directory\n",
    "base = os.getcwd()  \n",
    "# initialize the number of the first directory to be created\n",
    "t = 1\n",
    "# we use the previously created list of lines to get the urls we need\n",
    "scrapped_urls = lines[0:-1]\n",
    "for i in range(len(scrapped_urls)):\n",
    "    if(i%50==0):\n",
    "        # create a new folder\n",
    "        # remark: the newley created pages will start from 0\n",
    "        page_identifier = i-(50*t)\n",
    "        # subdirectory \n",
    "        directory = f\"page_{page_identifier}.html\"\n",
    "        # parent directories\n",
    "        parent_dir = base\n",
    "        # path\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        # make directory\n",
    "        os.makedirs(path)\n",
    "        # check\n",
    "        # print(\"Directory '%s' created\" %directory)\n",
    "        # change directory \n",
    "        os.chdir(path)\n",
    "        t += 1\n",
    "\n",
    "    # to avoid the issue with high frequency site-connections  \n",
    "    time.sleep(5)   \n",
    "\n",
    "    # get urls\n",
    "    URL = scrapped_urls[i]\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    # parsing\n",
    "    soup_data = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # saving\n",
    "    with open(f\"article_{i}.html\", \"w\") as file:\n",
    "        file.write(str(soup_data))\n",
    "        \n",
    "    # check\n",
    "    # print(f\"Article {i} chas been created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbdc2e",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ab9ff",
   "metadata": {},
   "source": [
    "At this point we have all the html documents about the animes of interest and we can start to extract the animes informations:\n",
    "- Anime Name (to save as `animeTitle`): String\n",
    "- Anime Type (to save as `animeType`): String\n",
    "- Number of episode (to save as `animeNumEpisode`): Integer\n",
    "- Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "- Number of members (to save as `animeNumMembers`): Integer\n",
    "- Score (to save as `animeScore`): Float\n",
    "- Users (to save as `animeUsers`): Integer\n",
    "- Rank (to save as `animeRank`): Integer\n",
    "- Popularity (to save as `animePopularity`): Integer\n",
    "- Synopsis (to save as `animeDescription`): String\n",
    "- Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "- Characters (to save as `animeCharacters`): List of strings.\n",
    "- Voices (to save as `animeVoices`): List of strings\n",
    "- Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147674ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "directory = 'html_pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff507d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(html_file_path):\n",
    "    \"\"\"\n",
    "    Function that extracts anime's informations.\n",
    "    Input: path (a string that is related to the position of each anime page in the folder tree)\n",
    "    Output: a list of lists with all the informations mentioned above\n",
    "    \"\"\"\n",
    "    \n",
    "    # take article_i.html from the directory \n",
    "    soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "    try:\n",
    "        animeTitle.append(str(soup.find_all('strong')[0].contents[0]))\n",
    "    except:\n",
    "        animeTitle.append('')\n",
    "\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            \n",
    "            # TYPES\n",
    "            if span.contents[0] == 'Type:':\n",
    "                try:\n",
    "                    animeType.append(str(div.find_all('a')[0].contents[0]))\n",
    "                except:\n",
    "                    animeType.append('NA')\n",
    "            \n",
    "            # NUMBER OF EPISODES\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                try: \n",
    "                    animeNumEpisode.append(int(div.contents[2]))\n",
    "                except:\n",
    "                    animeNumEpisode.append(0)\n",
    "            \n",
    "            # DATES\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                try:\n",
    "                    if len(div.contents[2]) > 21:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        end = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        endDate.append(end)\n",
    "                    else:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        endDate.append('-')\n",
    "                except:\n",
    "                        releaseDate.append('')\n",
    "                        endDate.append('')\n",
    "\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        \n",
    "        # MEMBERS\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        \n",
    "        \n",
    "        # SCORE\n",
    "        rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "        try:        \n",
    "            animeScore.append(float(rating.text.strip()))\n",
    "        except:\n",
    "            animeScore.append(None)\n",
    "\n",
    "     \n",
    "        # USERS\n",
    "        users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "        # here we we eliminate the word 'user '   \n",
    "        # that is why there is the [:-6] part\n",
    "        # we also replace the comma divisor\n",
    "        try:\n",
    "            animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        except:\n",
    "            animeUsers.append(0)\n",
    "\n",
    "\n",
    "        # RANK\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        try:\n",
    "            animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "        except:\n",
    "            animeRank.append(None)\n",
    "\n",
    "        # POPULARITY\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "    # DESCRIPTION\n",
    "    animeDescription.append(soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', ''))\n",
    "\n",
    "\n",
    "    # RELATED \n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    if(len(related)!=0):\n",
    "        x = []\n",
    "        y = []\n",
    "        for tr in related:\n",
    "            td = tr.find_all(\"td\")\n",
    "            for i in range(0, len(td), 2):\n",
    "                x.append(td[i].contents[0])\n",
    "                t = td[i+1].find_all(\"a\")\n",
    "                if(len(t[0].contents)!=0):  \n",
    "                    y.append(t[0].contents[0])\n",
    "                else:\n",
    "                    y.append(' ')\n",
    "            animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "    else:\n",
    "        animeRelated.append(' ')\n",
    "    \n",
    "    # CHARACTERS\n",
    "    try:\n",
    "        characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "        x = []\n",
    "        for i in chars:\n",
    "            x.append(i.contents[0].contents[0])\n",
    "        animeCharacters.append(x)\n",
    "    except:\n",
    "        animeCharacters.append(\" \")\n",
    "    \n",
    "   # VOICES\n",
    "    try:\n",
    "        voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "        y = []\n",
    "        for i in voices:\n",
    "            y.append(i.contents[1].contents[0])\n",
    "        animeVoices.append(y)\n",
    "    except:\n",
    "        animeVoices.append(\" \")\n",
    "    \n",
    "    # STAFF\n",
    "    try:\n",
    "        staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        staff = staff[1].find_all(\"td\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(1, len(staff), 2):\n",
    "            x.append(staff[i].contents[1].contents[0])\n",
    "            y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "        animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "    \n",
    "    except:\n",
    "        animeStaff.append(\" \")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE DIRECTORY ALREADY EXISTS THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution should take a few seconds\n",
    "\n",
    "# create a directory tree for .tsv files\n",
    "os.mkdir('tsv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE .tsv FILES ALREADY EXIST THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "def tsv_create(i):\n",
    "    \"\"\"\n",
    "    Function that creates a .tsv file form the html anime pages\n",
    "    Input: i, a positive integer\n",
    "    Output: empty\n",
    "    RemarK: it creates a .tsv file named anime_i in the tsv_files directory\n",
    "    \"\"\"\n",
    "    tsv_columns = ['animeTitle','animeType','animeNumEpisode','releaseDate','endDate','animeNumMembers','animeScore',\n",
    "                  'animeUsers','animeRank','animePopularity','animeDescription','animeRelated','animeCharacters',\n",
    "                  'animeVoices','animeStaff']\n",
    "    data = zip([animeTitle[i-1]],[animeType[i-1]],[animeNumEpisode[i-1]],[releaseDate[i-1]],[endDate[i-1]],[animeNumMembers[i-1]],[animeScore[i-1]],[animeUsers[i-1]],[animeRank[i-1]],[animePopularity[i-1]],[animeDescription[i-1]],[animeRelated[i-1]],[animeCharacters[i-1]],[animeVoices[i-1]],[animeStaff[i-1]])\n",
    "    tsv_file_name = 'tsv_files/anime_'+str(i)+'.tsv'\n",
    "    with open(tsv_file_name, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(tsv_columns)\n",
    "        for title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff in data:\n",
    "                tsv_output.writerow([title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE (WITH A SUBSET OF PAGES) \n",
    "# IF THE DIRECTORY AND THE .tsv FILES ALREADY EXIST THEN DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take quite a while (>1 hour)\n",
    "\n",
    "directory = 'html_pages'\n",
    "file_read = open('links.txt', 'r')\n",
    "anime_urls_list = file_read.readlines()\n",
    "file_read.close()\n",
    "\n",
    "for i in range(1,384):\n",
    "    html_page_name = 'page'+str(i+1)\n",
    "    directory_subfolder = directory+'/'+html_page_name+'/'\n",
    "    if(i!=383):\n",
    "        # 383th page has less than 50 animes\n",
    "        for j in range(1,51):\n",
    "            anime_num = 50*(i-1)+j\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            tsv_create(anime_num)\n",
    "    else:\n",
    "        for j in range(1,25):\n",
    "            anime_num = 50*(i-1)+j\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            tsv_create(anime_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018a5a8",
   "metadata": {},
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d7c3b",
   "metadata": {},
   "source": [
    "We will create two different Search Engines that, given as input a query, return the animes that match the query. First, we need to pre-process all the information collected for each anime by:\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "\n",
    "For this purpose, we will use the `nltk` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c29da",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73781b",
   "metadata": {},
   "source": [
    "For the first version of the search engine, we narrow our interest on the `Synopsis` of each anime. It means that we will evaluate queries only with respect to the anime's description (and `Title` as we believe it is also an important part of an anime description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords and store them in a variable\n",
    "stop = stopwords.words('english')\n",
    "# stemmer\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe379de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    \"\"\"\n",
    "    Input: sentence, a string\n",
    "    Output: tokenized sentence\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0419ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "stem_sentences('playing Tennis and Golf all the day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTitle_list = []\n",
    "animeDescription_list = []\n",
    "\n",
    "# create a lists from tsv files\n",
    "for i in range(0,19124):\n",
    "    anime_tsv = open('tsv_files/anime_'+str(i+1)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "    data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "    data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "    animeTitle_list.append(str(data.animeTitle[0]))\n",
    "    animeDescription_list.append(str(data.animeDescription[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate lists to create a dataframe\n",
    "anime_df = pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list]), \n",
    "                               columns=['animeTitle', 'animeDescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords from the dataframe\n",
    "anime_df['animeDescription']  = anime_df['animeDescription'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# removing punctuations from the dataframe\n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# stemming the dataframe \n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].apply(stem_sentences)\n",
    "\n",
    "# remove [Written by MAL Rewrite]\n",
    "# e.g. text.replace('[Written by MAL Rewrite]', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = ' '.join([i for i in anime_df['animeDescription']]).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d6c38",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d15f",
   "metadata": {},
   "source": [
    "Given a query (e.g. *saiyan race*) the Search Engine returns a list of documents. Since we are dealing with conjunctive queries (AND), each of the returned documents contains all the words in the query. The final output of the query returns, if present, the following information for each of the selected documents:\n",
    "- animeTitle\n",
    "- animeDescription\n",
    "- Url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "words_dict = set(words_list)\n",
    "\n",
    "# assign a unique integer id to each unique word\n",
    "vocabulary = {}\n",
    "i=1\n",
    "for word in words_dict:\n",
    "    vocabulary.update({i:word})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca106c",
   "metadata": {},
   "source": [
    "### 2.1.1 Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE vocabulary.json FILE EXISTS DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# create the vocabulary.json file to store each unique word \n",
    "# and its corresponding id number \n",
    "with open(\"vocabulary.json\", \"w\") as file:\n",
    "    json.dump(vocabulary, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE ONLY ONCE\n",
    "# IF THE inverted.json FILE ALREADY EXISTS DO NOT EXECUTE THIS CELL\n",
    "\n",
    "# REMARK: the execution can take some time (>1 hour)\n",
    "\n",
    "# create inverted index\n",
    "inverted_dict = {}\n",
    "with open('vocabulary.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    for key, value in tqdm(data.items()):\n",
    "        inverted_list = []\n",
    "        for i in range(0,len(anime_df)):\n",
    "            if(value in anime_df['animeDescription'][i].split()):\n",
    "                anime_name = 'anime_'+str(i+1)\n",
    "                inverted_list.append(anime_name)\n",
    "                inverted_dict.update({key:inverted_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE IF AND ONLY IF THE inverted.json FILE ALREADY EXISTS \n",
    "\n",
    "# save the inverte index in a .json file\n",
    "with open(\"inverted.json\", \"w\") as file:\n",
    "    json.dump(inverted_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e7cf4",
   "metadata": {},
   "source": [
    "### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce810b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_query(query_list):\n",
    "    \"\"\"\n",
    "    Input: the user's query, a string\n",
    "    Output: a list of animes that match the query\n",
    "    \"\"\"\n",
    "    anime_query_list = []\n",
    "    for word in query_list:\n",
    "        with open('vocabulary.json') as data_file:\n",
    "            data = json.load(data_file)\n",
    "            for key, value in data.items():\n",
    "                if(word == value):\n",
    "                    with open('inverted.json') as inverted_file:\n",
    "                        inverted_data = json.load(inverted_file)\n",
    "                        for inv_key, inv_value in inverted_data.items():\n",
    "                            if(key == inv_key):\n",
    "                                # appending the value to a list if has the specific query word\n",
    "                                anime_query_list.append(inv_value)\n",
    "    \n",
    "    # creating a list from all animes including duplicate ones\n",
    "    anime_list = []\n",
    "    for i in range(len(anime_query_list)):\n",
    "        for j in range(len(anime_query_list[i])):\n",
    "            anime_list.append(anime_query_list[i][j])\n",
    "    \n",
    "    # creating a set to find non duplicate anime files\n",
    "    anime_query_set_list = list(set(anime_list))\n",
    "    # creating an empty list to store the final anime list which has all the input queries \n",
    "    anime_final_list = []\n",
    "    \n",
    "    # counting the occurences of each anime with the length of the total query\n",
    "    # if its equal to total len, then each word in the query appears on the anime description\n",
    "    for anime in anime_query_set_list:\n",
    "        if(anime_list.count(anime) == len(anime_query_list)):\n",
    "            anime_final_list.append(anime)\n",
    "    return anime_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b581352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_anime_df(anime_list):\n",
    "    \"\"\"\n",
    "    Input: a list of animes (obteined through a user's query)\n",
    "    Output: a dataframe with the title, the description and the url of all the\n",
    "            animes in the list\n",
    "    \"\"\"\n",
    "    # creating lists for animes\n",
    "    animeTitle_list = []\n",
    "    animeDescription_list = []\n",
    "    animeUrl_list = []\n",
    "\n",
    "    # assigning tsv values from the animes to lists we've just created \n",
    "    for anime in anime_list:\n",
    "        anime_tsv = open('tsv_files/'+anime+'.tsv', 'r',encoding=\"utf8\")\n",
    "        data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "        data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "        data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "        animeTitle_list.append(str(data.animeTitle[0]))\n",
    "        animeDescription_list.append(str(data.animeDescription[0]))\n",
    "\n",
    "    # reading text file url lines to a list\n",
    "    f=open('links.txt')\n",
    "    url_lines=f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # creating a for loop to iterate over each anime we have on the anime_list\n",
    "    for anime in anime_list:\n",
    "        # getting the int value from the anime name\n",
    "        anime_num=(int(anime.split(\"anime_\",1)[1]))\n",
    "        # finding the corresponding line from the links.txt and assigning it to a list\n",
    "        animeUrl_list.append(url_lines[(anime_num-1)])\n",
    "\n",
    "    # creating the dataframe from lists and returning it\n",
    "    return pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list, animeUrl_list]), \n",
    "                                   columns=['animeTitle', 'animeDescription', 'Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13de143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting an input from the user\n",
    "# example\n",
    "# query = 'saiyan race'\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# getting the list of animes which has the query\n",
    "anime_list = find_query(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the anime dataframe from our query\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a9781",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd682c",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c47c5f",
   "metadata": {},
   "source": [
    "**Disclamair**: we took and adapted some of the following coding ideas from https://www.geeksforgeeks.org/k-maximum-sums-non-overlapping-contiguous-sub-arrays/ and also from the discussions on\n",
    "https://www.hackerrank.com/challenges/maximum-subarray-sum/problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae06d9",
   "metadata": {},
   "source": [
    "\n",
    "Consult for managing back-to-back sequences of requests for appointments. A sequence of requests is of the form `[30, 40, 25, 50, 30, 20]` where each number is the time that the person who makes the appointment wants to spend. Aaccept some requests with a break between them. Two consecutive requests are not accepptable. \n",
    "\n",
    "For example, `[30, 50, 20]` is an acceptable solution (of duration 100), but `[30, 40, 50, 20]` is not, because 30 and 40 are two consecutive appointments. \n",
    "\n",
    "**Goal**: provide a schedule that maximizes the total length of the accepted appointments. Provide also:\n",
    "- an algorithm that computes the acceptable solution with the longest possible duration;\n",
    "- a program that given in input an instance in the form given above, gives the optimal solution\n",
    "\n",
    "For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration 110."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692a707",
   "metadata": {},
   "source": [
    "## Formalization of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8abb8e",
   "metadata": {},
   "source": [
    "Given an array of positive integers, find the maximum sum of all the subsequences with the constraint that no two numbers in the subsequences are adjacent in the array and return both the maximum sum and the subsequence(s) that realize the maximum sum. If $f=f(v)$ is the function we want to implement and $v=(30, 40, 25, 50, 30, 20)$, then we should have $f(v)=(40, 50, 20)$ with sum $s=110$, as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ed821",
   "metadata": {},
   "source": [
    "**Algorithmic idea: Dynamic programming**. Given an array $v$, let $v^*[i]$ be the optimal solution using the elements with indices $0,..,i$. In order to have a recursive algorithm that terminates set $v^*[0] = v[0]$, and $\\max(v[0],v[1])=0$, then $v^*[i] = \\max(v^*[i - 1], v^*[i - 2] + v[i])$ for $i = 1, ..., n$ (where $n$ is the dimension of the array given in input). Clearly $v^*[n]$ is the solution we want and it is obteined in $O(n)$. We can then use another array to store which choice is made for each subproblem, and so recover the actual elements chosen.\n",
    "\n",
    "The same idea can be used to solve a more general problem as shown in the examples at the end of this paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e19b1",
   "metadata": {},
   "source": [
    "*Example* . Let $v=(1,2,2,10,1)$ and consider the matrix \\begin{pmatrix} 1 & 0+2=2 & \\dots & 12 & 4 \\\\ 0 & \\max(0,1)=1 & \\dots &3 & 12  \\end{pmatrix}\n",
    "\n",
    "then the maximum subsequence with no adjecent elements sum is 12 and the elements that realize it are (2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f36bc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dec1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to initialize dictionaries with a lambda function \n",
    "# and provides the default value for a nonexistent key.\n",
    "# so a defaultdict will never raise a KeyError.\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b95a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(array):\n",
    "    # to track sums\n",
    "    sums = [0]*len(array)\n",
    "    \n",
    "    # to track elements of the input array\n",
    "    # example: if array = [1,2,3,5,4] at the emd of the following for loop\n",
    "    # elements = {(0, 1): 1, (0, 2): 2, (1, 3): 4, (2, 5): 7, (4, 4): 8}\n",
    "    elements = defaultdict(lambda: -1)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        # calculate maximum sum \n",
    "        sums[i] = max(sums[i-1], sums[i-2] + array[i])\n",
    "        # memorize\n",
    "        if max(sums[i-1], sums[i-2] + array[i])- (sums[i-2] + array[i]) == 0:\n",
    "            elements[sums[i-2], array[i]] = sums[i]\n",
    "    \n",
    "    # retrieve elements that produce the optimal solution\n",
    "    optimal_subarray = []\n",
    "    \n",
    "    # inizialization\n",
    "    max_value = max(elements.values())\n",
    "    count = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "    \n",
    "    \n",
    "    # to print the optimal subarray\n",
    "    # example: if elements = {(15, 11): 26} it means that 15 is the cumulative sum\n",
    "    # in this case 15 = 2+5+4+4 and (2,5,4) is the optimal solution, and 11 is the optimal subsequence sum\n",
    "    # the values stored in the second index are those we need, and the first index we use it to check\n",
    "    # when there are no more elements (i.e. count = 0)\n",
    "    while count != 0:\n",
    "        optimal_value = list(elements.keys())[list(elements.values()).index(max_value)][1]\n",
    "        cum_sum = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "        # put an element that realizes the optimal solution to the list\n",
    "        optimal_subarray.insert(0,optimal_value)\n",
    "\n",
    "        max_value = cum_sum\n",
    "        count = cum_sum\n",
    "\n",
    "    \n",
    "    return optimal_subarray, sums[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f534e1",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84f47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 10], 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,2,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87823ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 5, 4], 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,3,5,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8968a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([40, 50, 20], 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([30, 40, 25, 50, 30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a3b7f",
   "metadata": {},
   "source": [
    "## Solution of a generalization of the previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87eb64",
   "metadata": {},
   "source": [
    "**Attention:** the following code needs refinement. For example it works poorley in some test cases (e.g. when in the array there are duplicate elements or a lot of contiguous elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacdd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(lambda: -1)\n",
    "prefix_sum = []\n",
    "trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac913c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array_sum(i, j):\n",
    "    \"\"\"\n",
    "    Input: indexes i,j of an array v with i<j\n",
    "    Output: v[i]+v[i+1]+...+v[j-1]+v[j]\n",
    "    Remark: if i>j returns 0\n",
    "    \"\"\"\n",
    "    if i == 0:\n",
    "        return prefix_sum[j]\n",
    "    return (prefix_sum[j] - prefix_sum[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ca8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_sum(cur, v, k):\n",
    "    \"\"\"\n",
    "    Input: current element cur, array v, positive integer k \n",
    "    Output: current maximum sum \n",
    "    Remark: this function allows also track the elements that realise the maximum sum.      \n",
    "    \"\"\"\n",
    "    if cur >= len(v):\n",
    "        return 0\n",
    "    if dd[cur] != -1:\n",
    "        return dd[cur]\n",
    "    \n",
    "    # use the following line when all the elements in the array are positive, \n",
    "    # else set s1 and s2 to -Infinity\n",
    "    s1 = -1; s2 = -1\n",
    "    \n",
    "    # choose subarray starting at the current element \"cur\"\n",
    "    if cur + k - 1 < len(v):\n",
    "        # Remark: sub_array_sum(cur,cur)=0\n",
    "        s1 = sub_array_sum(cur, cur + k - 1) + maximum_sum(cur + k + 1, v, k)\n",
    "    \n",
    "    # ignore subarray starting at \"cur\"\n",
    "    s2 = maximum_sum(cur + 1, v, k)\n",
    "    dd[cur] = max(s1, s2)\n",
    "    \n",
    "    if s1 >= s2:\n",
    "        # keep track of the elements that realise the maximum sum\n",
    "        trace[cur] = (True, cur + k + 1)\n",
    "        return s1\n",
    "    trace[cur] = (False, cur + 1)\n",
    "    \n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb106191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array(v, trace, k):\n",
    "    \"\"\"\n",
    "    Input: array v, array trace, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray\n",
    "    Remark: this function allows to return non-consecutive subarrays of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    subArrays = []\n",
    "    for i in range(len(trace)):\n",
    "        if trace[i][0]:\n",
    "            subArrays.append(v[i : i + k])\n",
    "        i = trace[i][1]\n",
    "\n",
    "    return subArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc94b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_solution(v, k):\n",
    "    \"\"\"\n",
    "    Input: array v, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray(s)\n",
    "    Remark: this function allows to return non-consecutive optimal subarray(s) of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    global dd, trace, prefix_sum\n",
    "    dd = defaultdict(lambda: -1)\n",
    "    \n",
    "    # initialization\n",
    "    trace = [(False, 0)] * len(v)\n",
    "    prefix_sum = [0] * len(v)\n",
    "    prefix_sum[0] = v[0]\n",
    "    \n",
    "    for i in range(1,len(v)):\n",
    "        prefix_sum[i] += prefix_sum[i - 1] + v[i]\n",
    "        \n",
    "    print(\"Array :\", v)\n",
    "    print(\"Max sum: \", maximum_sum(0, v, k))\n",
    "    print(\"Subarrays: \", sub_array(v, trace, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65bd5d",
   "metadata": {},
   "source": [
    "## Some examples of solution of a more general problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b224df3",
   "metadata": {},
   "source": [
    "To sole a generalized version of the problem take $k>1$, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e97123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  9\n",
      "Subarrays:  [[1], [3], [5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab820c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[1, 2], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026c57c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f3347",
   "metadata": {},
   "source": [
    "## Alternative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3f428",
   "metadata": {},
   "source": [
    "With immense surprise we have found that it is possible to solve the problem with just 3 lines of code! See https://codegolf.stackexchange.com/questions/183390/maximum-summed-subsequences-with-non-adjacent-items?answertab=active#tab-top for more deatils. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe2f97",
   "metadata": {},
   "source": [
    "Here it is the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90dd15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [30, 40, 25, 50, 30, 20]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6479130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 50, 20] 110\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c2ec0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 2, 3, 5, 4]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2539d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4] 8\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef307e",
   "metadata": {},
   "source": [
    "**Credits**: Chas Brown https://codegolf.stackexchange.com/users/69880/chas-brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900dd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
